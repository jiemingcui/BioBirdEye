{
    "identity": "pex-1836",
    "title": "<p>Global Tracking of Climate Change Adaptation Policy Using Machine Learning: a Systematic Map Protocol</p>",
    "content": [
        {
            "header": "Introduction",
            "content": "<p><em>NB: our protocol is more extensive than required by Protocol Exchange and not all parts fit in the Protocol Exchange's pre-determined categories. In addition, tables and figures cannot be inserted in-text. We therefore highly recommend that you read the formatted document instead, which can be found under the Supplementary Files section. </em></p><p><br></p><p><strong>1. Introduction</strong></p><p>With the impacts of climate change becoming increasingly clear and pressing (IPCC, 2018, IPCC, 2021), adaptation to these impacts has become an important focus of climate policy, especially since the Paris Agreement (Lesnikowski et al., 2017). In the Agreement, countries commit to additional reporting, including a Global Stocktake on Adaptation, with the goal of assessing global progress and the adequacy and effectiveness of adaptation measures. For mitigation, such efforts are perhaps best summed up by the assessment of the \u201cemissions gap\u201d (United Nations Environment Programme, 2021b) which provides clear guidance on the progress of countries in reducing emissions and the size of the remaining challenge. Quantifying the analogous \u201cadaptation gap\u201d (United Nations Environment Programme, 2021a) however has proven to be more difficult.</p><p>These difficulties are in part caused by fundamental disagreements on the meaning of adaptation and whether it can be meaningfully measured (Arnott et al., 2016, Runhaar et al., 2018, Craft and Fisher, 2018). More practically, adaptation tracking systems are not present in the majority of countries, even if they have a national adaptation plan (United Nations Environment Programme, 2021a, Leiter, 2021) and national tracking systems are often not designed for to support global comparisons (Magnan and Chalastani, 2019). This despite Global Stocktake\u2019s reliance on country reports (Christiansen et al., 2020, Craft and Fisher, 2018). Independent efforts to create an overview of adaptation progress face a complex task as policies range from the international to the municipal level (Berrang-Ford et al. 2019; Olazabal et al. 2019); Persson and Dzebo (2019) even state that global or transnational adaptation governance is so ill-defined that it \u201ccan become an impossibly broad phenomenon to study\u201d (p. 358). Adding to the challenge is the lack of a central database to record policies \u2013 though some efforts do exist (Nachmany et al., 2019, F\u00fcssel and Almond, 2021). Systematic assessments of the scientific literature meanwhile provided some insight in the past (Berrang-Ford et al., 2011), but since then, the sheer volume and heterogeneity of adaptation-relevant research has increased markedly, complicating synthesis efforts (Sietsma et al., 2021).</p><p>These challenges are not unique to the adaptation community. Spreading out from computer science, the term \u201cBig Data\u201d has been widely adopted, taking on various meanings along the way (Kitchin and McArdle 2016), all centred around the use of computers to extract meaningful insights from data with a high volume, velocity, and variety (Laney 2001). In recent years, such approaches have also made their way into climate change research (Rolnick et al., 2019). Some climate change researchers have used machine learning approaches to build and analyse large text-based datasets (Hsu and Rauber, 2021, Lesnikowski et al., 2019a, Biesbroek et al., 2020, Callaghan et al., 2020, Lamb et al., 2019). Relatedly, the phenomenon of \u201cBig Literature\u201d (Nunez-Mir et al., 2016), a rapid increase in the number of scientific publications, has led some within the evidence synthesis community to integrate machine learning methods into a more traditional systematic review process (Haddaway et al., 2020, Nakagawa et al., 2019, Van de Schoot et al., 2021). This includes the use of machine learning to create \u201cevidence maps\u201d; for example, to identify trends and research gaps on health impacts of climate change (Berrang-Ford et al., 2021c), to assess progress within climate change adaptation research (Sietsma et al., 2021) or to categorise and estimate the size of climate change impacts worldwide (Callaghan et al., 2021).</p><p>Despite the promise of machine learning methods, they are rarely applied in an adaptation policy context (notable exceptions: Biesbroek et al., 2022, Biesbroek et al., 2020, Lesnikowski et al., 2019a, Ulibarri et al., 2022). Moreover, the discourse around the Global Stocktake is mostly devoid of any substantive discussion on computer-based methods \u2013 even the Adaptation Committee (2020) limits their discussion of Big Data to one text box (p. 28), which is largely filled with explanations of definitions and refers to a few data initiatives outside of climate change. This does not do justice to the proven track record of machine learning methods in other closely related contexts and applications \u2013 in fact, it is <em>less</em> substantive than earlier calls for Big Data applications for adaptation (Ford et al., 2016).&nbsp;</p><p>The lack of progress may simply be a reflection of the lack of overlap between the people and expertise in both research communities (Lesnikowski et al., 2019a, Haddaway et al., 2020); generally, adaptation researchers appear to favour in-depth qualitative approaches (Ford and Berrang-Ford, 2016). However, given the rapidly increasing pace of adaptation publications (Wang et al., 2018, Haunschild et al., 2016), adaptation researchers may ultimately be forced to use machine learning tools to synthesise research (Sietsma et al., 2021).</p><p>In the research outlined in this protocol, we adapt methods from other climate-relevant evidence maps (chiefly, Callaghan et al., 2021, Sietsma et al., 2021) to address the challenge of adaptation tracking. We do so by:&nbsp;a) training a machine learning algorithm to recognise when scientific papers are discussing adaptation policy; b) using machine learning to categorise these papers to connect them to both established policy research and to active debates among adaptation researchers; and c) using further Natural Language Processing (NLP) and evidence mapping techniques to visualise developments over time and geographic distribution of the evidence.</p><p>Following best practice for evidence mapping in the environmental sciences, we design our protocol around the ROSES standard (Haddaway et al., 2018). We deviate from this standard in three key instances to better fit the machine learning approach of this project: first, we will not retrieve full texts of our documents, relying instead on title and abstracts, as frequently done in machine-learning assisted systematic maps, because they condense key information and avoid excessive type-II errors (Marshall and Wallace, 2019, Callaghan et al., 2021); secondly, not all documents will be screened by hand, so we will describe instead how the typology was operationalised for human coders to label a subset of the documents (the \u201ctraining set\u201d), from which a supervised machine learning algorithm will then learn to categorise the full dataset (Marshall and Wallace, 2019, Callaghan et al., 2021, Berrang-Ford et al., 2021a); and third, as machine learning allows for a more quantitative estimate of both comprehensiveness and errors, we will discuss how these relate to the size of the training set and the interpretation of results.</p><p><br></p><p><strong>2. Stakeholder Engagement</strong></p><p>There are three main phases of stakeholder engagement within this project: writing this protocol, gathering feedback on our methods, and disseminating the results. We rely mostly on the breadth of experience of our team, but will aim to engage stakeholders from both the adaptation community <em>and </em>the machine learning/evidence synthesis community.</p><p>The project team includes researchers from a wide array of backgrounds. This includes university-based researchers on adaptation (JDF, IVC), public policy (RB, JCM), and machine learning methods for climate change research (MC, JCM, AJS). Further expertise comes from the researchers based at Climate Analytics (CS, ET, AT), which, as an institute, specialises in bringing cutting-edge climate science into the policy debate.</p><p>All team members contributed to writing this protocol. The protocol will be publicly accessible and open for feedback.</p><p>Once complete, the research will again be made public, including the data and code. The hope is that both the dataset and the trained algorithms can provide a steppingstone for further research into climate change adaptation policy. To this end, we hope to present the results at scientific conferences, though the current pandemic makes that we cannot make our plans as concrete as we would wish. The quality of the final dataset will obviously also influence uptake by other stakeholders; if the data has sufficient detail, it may be worthwhile to create an easily accessible platform, which would require a new round of stakeholder engagement to determine their usability needs.</p><p><br></p><p><strong>3 Objective</strong></p><p>The overall objective is to systematically map evidence on climate change adaptation policies in the global academic literature, with a focus on the geographic distribution of different policy types and their development over time.</p><p><br></p><p><u>3.1&nbsp;&nbsp;&nbsp;Research questions</u></p><p>We aim to create an evidence map of the scientific literature on climate change adaptation policy with global coverage. To link this to established policy research, our analysis will centre around the NATO typology \u2013 Nodality, Authority, Treasure and Organisation (Hood, 1986); we will also address issues of contention within the scientific literature on adaptation, specifically:</p><p>\u25cf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Developments over time \u2013 is there an increase in adaptation policy?</p><p>\u25cf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Geographic locations \u2013 how are adaptation policies distributed worldwide?</p><p>\u25cf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Alignment to policy culture \u2013 do countries use similar policy tools for their adaptation policy as they are for their non-adaptation policies?</p><p>\u25cf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Alignment to impacts \u2013 do the adaptation polices respond to the predicted or experienced impacts of climate change within a region or area?</p><p>\u25cf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Constraints and limits \u2013 do we see increased attention to the constraints and limits of climate change adaptation, and if so, what types of constraints are most prevalent?</p><p>\u25cf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Maladaptation \u2013 some adaptation actions may have unintended negative consequences, which can end up increasing vulnerability to climate change; how prevalent is this issue?</p><p><br></p><p><u>3.2&nbsp;&nbsp;&nbsp;Problem scope</u></p><p>In general, for this project, we aim to be as comprehensive as possible: we wish to capture the full breadth of scientific research that has elements of both climate change adaptation and some form of policy- or policy-enabled response. The use of machine learning is crucial to taking such a broad view.</p><p>At the same time, we must take care not to cast too wide a net because even with the help of machine learning, we cannot meaningfully analyse too large a dataset. For this reason, we will limit our analysis to those papers which explicitly mention both elements \u2013 i.e. they must include a substantive analysis of both adaptation <em>and </em>policy. More details on the scope of the evidence considered is given in Table 1, along the Population, Intervention, Context, Timeframe (PICoST) framework for systematic reviews.</p><p><br></p><p><em>3.2.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Adaptation</em></p><p>Adaptation<strong> </strong>here is defined in line with the IPCC to include all actions which aim to adjust to the actual or expected climate and its effects. We limit ourselves to adaptation in human systems, where adaptations seek to moderate or avoid harm or exploit beneficial opportunities caused by climate change. Crucially, this includes literature where neither the words \u201cclimate change\u201d nor \u201cadaptation\u201d are used by the authors. Climate change has many different impacts, but the link of climate change may not always be known to the authors, or it may be omitted for political expediency-- e.g. literature on improved irrigation may write about prolonged droughts without using the words climate change. To determine whether an impact is attributable to climate change, we will follow the latest report by the Intergovernmental Panel on Climate Change (IPCC, 2021).</p><p><br></p><p><em>3.2.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Policy</em></p><p>For the purposes of this review, we refer to (public) policies as the outputs public actors that are designed to achieve defined goals and solutions to societal problems (Knill and Tosun, 2020). These outputs can take different forms, including programs, measures, decisions, legislation, strategies and other courses of action. Although there are different ways to study public policy, we focus on the underlying governance principles and instruments used by governments to achieve their goals.</p><p>Several typologies exist to classify the types of tools used by governments, see Capano and Howlett (2020). Here we adopt the frequently used NATO-typology: Nodality, Authority, Treasure, and Organization (Hood, 1986). This typology has also been used in an adaptation context, for example by Henstra (2017) in a Canadian case study, by Lesnikowski et al. (2019b) to analyse the adaptation policy instrument mixes of 125 local governments, and by Biesbroek and Delaney (2020) to systematically map 183 adaptation policy studies in Europe.</p>"
        },
        {
            "header": "Reagents",
            "content": ""
        },
        {
            "header": "Equipment",
            "content": ""
        },
        {
            "header": "Procedure",
            "content": "NB: as noted above, our protocol is more extensive than required by Protocol Exchange and not all parts fit in the Protocol Exchange's pre-determined categories. In addition, tables and figures cannot be inserted in-text. We therefore highly recommend that you read the formatted document instead, which can be found under the Supplementary Files section. Protocol exchange requests a list here, which we will therefore give first:\n1) retrieve documents from scientific databases;\n2) manually label a subset of these documents;\n3) use the labelled documents to train a binary classifier which selects relevant literature;\n4) assign categories to the documents, using a supervised multi-label classifier, topic modelling, and a geoparser;\n5) combine the different results of the previous step to synthesise the data\n4 Methods\n4.1\u00a0\u00a0\u00a0A machine learning approach to evidence mapping\nAs described by Haddaway et al. (2020), a systematic map or evidence map aims to summarise an entire evidence base. Where a systematic review aims to combine the findings of individual studies (\u201cwhat works where and how?\u201d), an evidence map by contrast aims to describe what is known in an area (\u201cwhat kinds of research exist?\u201d). Both systematic reviews and maps share a focus on transparent and robust methods; they lay out the rationale for their main research questions in a protocol, which also describes the main methods, as we do here. The basis for the map is an often extensive search query; matching documents are then screened by hand, discarding irrelevant documents and dividing relevant documents into categories. Combined with meta-data, this allows researchers to describe developments in the field. Results often take the form of a searchable or interactive database, as well as visualisations of the data, a list of knowledge gaps and clusters, and a report noting key findings.\nSystematic maps in particular may benefit from incorporating computational methods, given their relatively descriptive nature and considerable evidence base (Haddaway et al., 2020). In this project, we use the ability of computers to handle large amounts of data to create an evidence map that is as inclusive as possible. This means that the documents cannot reasonably be screened by hand, which is why we make use of so-called \u201csupervised machine learning\u201d to both select and classify documents. Although we cannot provide a full introduction to machine learning here (we will instead refer the interested reader to the introduction provided by Marshall and Wallace, 2019), it is important to understand that supervised machine learning in essence attempts to mimic human decision making. It does so based on the so-called \u201ctraining set\u201d, which contains data labelled by humans. For this project,\u00a0no appropriate pre-labelled dataset exists. A large proportion of this protocol is therefore dedicated to describing how we will create our training set, from which the algorithms will \u201clearn\u201d how to select relevant documents and which categories these documents belong to\n.\nWe make use of additional machine learning methods to extract further information from our dataset and combine the different layers of information for our final assessment.\nAltogether, our approach can be divided into 5 stages, given at the outset -- see also Figure 1. Conceptually, this is similar to earlier computer-assisted evidence maps (including work by members of the project team, e.g. Callaghan et al., 2021, Callaghan et al., 2020, Sietsma et al., 2021).\nIn the next section, we will detail the search strategy for step 1. We then describe the criteria used to create the training set used in steps 3 and 4 in section\n4.3 Article selection and classification\n. More details on our chose machine learning methods are\nin 4.5 Machine learning considerations.\nThe other machine learning methods used for categorisation \u2013 i.e. the geoparser and topic modelling \u2013 do not make use of the pre-labelled data. More details on their use are found in section\n4.6 Data extraction.\nThe final section,\n4.7 Data Synthesis,\ndescribes how we create our evidence map with knowledge gaps and clusters.\n4.2\u00a0\u00a0\u00a0Search strategy\n4.2.1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Source\nThe search will be carried out on the Web of Science Core Collection\n[1]\n, which is a publisher-independent scientific database with a wide coverage. In addition, we will also search the MEDLINE database, which covers life sciences and biomedical research, as well as Scopus, which is maintained by the publisher Elsevier and contains a variety of high-quality journals. Note that these databases have a bias towards natural sciences, as well as towards English-speaking countries and the Global North (Mongeon and Paul-Hus, 2016, Vera-Baceta et al., 2019), which limits the representativeness of our results.\nThe search will be carried out on title, abstract and keywords for all databases, as well as Keywords Plus for Web of Science. We will not limit the search by date, but will limit our search to articles and reviews, in line with our problem scope.\nAs is common for a machine learning based screening process (Marshall and Wallace, 2019, Haddaway et al., 2020), we will not assess any full texts. Full text screening would be time-intensive (Haddaway and Westgate, 2019), whereas we will need a substantial number of hand-coded documents for our machine learning algorithms to function. We will therefore retrieve only abstracts, titles and meta-data (publication year, authors, author affiliations, keywords, references, field of research for Web of Science) for each article matching our search string; this information per article is what is meant by the word \u201cdocument\u201d.\n[1]\nWeb of Science Core Collection here includes:\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Science Citation Index Exvpanded (SCI-EXPANDED) --1900-present\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Social Sciences Citation Index (SSCI) --1900-present\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Arts & Humanities Citation Index (A&HCI) --1975-present\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Conference Proceedings Citation Index- Science (CPCI-S) --1990-present\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Conference Proceedings Citation Index- Social Science & Humanities (CPCI-SSH) --1990-present\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Emerging Sources Citation Index (ESCI) --2015-present\n4.2.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Search string\nOur search string has three main components: climate change, adaptation and policy. Each of these parts in turn has multiple sub-components for which a query was constructed. Documents need to match at least one keyword from all strings -- i.e. they are linked by a boolean AND. The majority of sub-components are internally linked by a boolean OR.\nOur broad climate search string is a modified version of the query used by (Callaghan et al., 2020), which in turn is based on Grieneisen and Zhang (2011). We remove the majority of mitigation-related terms, but expand the general climate change part, and add keywords on impacts, vulnerability and risk. Note this general part of the query captures all literature which explicitly mentions \u201cclimate change\u201d already. The added terms around climate impacts are based on the IPCC\u2019s AR6 Table 12.2. This table describes changes in natural systems and impacts on human and natural systems that can be at least partially attributed to climate change. By including this, we capture literature which does not mention climate change explicitly, but does contain responses to impacts that are primarily driven by climatic changes.\nIn the adaptation component of our query, we take a similar approach: in line with our Problem Definition above, we need to strike a balance, such that we also capture the wider literature where climate change is a recognised contributor to the subject of interest, without capturing the much larger literature that investigates weather phenomena, rather than climate change. To do so, we split this part of the query in three parts: 1) recognised changes in natural systems based on the aforementioned table in AR6; 2) recognised impacts of these changes based on this same table; and 3) recognised adaptive responses based on the Final Government Draft of the IPCC AR6 WG2\u2019s Cross Chapter Box FEASIB, which lists responses to climate change; given that the full WG2 report was not available to us at the time of publishing this protocol, the response options mentioned in AR 5 table 14.1 are added to this. The second and third part here link general weather terms with a boolean AND to the impact and response keywords respectively. These general weather terms are a wider version of the same impacts covered under the first part.\nFinally, in the policy part of our components, we again take a broad view of what could be considered relevant. We include terms around policy and governance, including key moments for adaptation in the UNFCCC process. Some governance-related terms (e.g. framework, management/managing) are so widely used that they no longer proved sufficiently selective on their own; similarly, keywords around governance levels (e.g. national/international, cities) also proved too general. We therefore combine these two types of keywords with the NEAR operator. This means that for example articles mentioning a \u201cnational framework\u201d are still captured.\nThe search terms are given in Web of Science syntax in table in the full document. The plain-text search strings are given in Supplementary Materials 2.\n4.23\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Comprehensiveness of the search\nTwo main factors limit the comprehensiveness of our search strategy. Both are common limitations for works of this kind (Konno and Pullin, 2020). First, keywords are English only, so we will only capture non-English literature that is indexed in English. Combined with the language bias of the databases (Vera-Baceta et al., 2019), this will lead to a relative over-representation of literature from English-speaking countries. Second, our database does not include grey literature. Although it seems likely that a substantial amount of such literature is relevant to adaptation policy, it is more difficult to access in a reproducible manner and often will not include an abstract. The latter poses a problem for us, as our machine learning will take place at the abstract level. In addition to the different format, the lack of a comprehensive database of grey literature also would make its inclusion more complex and time-intensive. As such, this is left to future projects.\nThe above two caveats notwithstanding, we will strive for search terms and document selection that are as close to comprehensive as possible. The following factors should help ensure this:\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0The project team consists of a diverse range of adaptation researchers, with backgrounds ranging from engineering to policy research. This team has all been involved in the formulation of the search query and coding guidelines. A diverse subset will be involved in the coding itself.\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0This protocol will be made public prior to completing the work.\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0We will cross-check the resulting documents against the references of the Special Report on 1.5C (IPCC, 2018) and the latest Adaptation Gap Report (United Nations Environment Programme, 2021a). We will then scan the list of missing articles for titles that appear adaptation relevant to identify which keywords might be missing.\nWe will not update the search beyond this within the current project. However, we wish to stress that, once the algorithm is trained, it can be used to filter future searches too. This means that, given proper support, this project could be used as the basis for a so-called \u201cliving evidence map\u201d which automatically updates as new scientific papers are published.\n4.2\u00a0\u00a0\u00a0Article selection and classification\n4.2.1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Screening strategy\nThe above search strategy is purposefully kept broad, meaning it includes both relevant and irrelevant articles. We will make use of supervised machine learning to first select the subset that is relevant to climate change adaptation, and then to classify these relevant articles into different categories.\nIn practice, this leads us to a three-tiered approach for article selection.\nIn the first step,\nrelevant articles are selected. We first check the articles basic bibliographic data against our criteria set out in the PICoST framework outlined earlier, selecting only articles and reviews and filtering out articles where the abstract is missing. After this, a group of coders will determine whether the document is relevant. There are only two possible labels here: relevant or irrelevant. To be considered relevant, the document should meet two content-based criteria:\nThe document must include a substantial focus on a response to climate change or to a weather phenomenon wherefore changes can confidently be attributed to climate change, as determined by the IPCC.\nThe adjustment must be either enabled by, supported by, or a direct result of at least one policy.\nThese inclusion/exclusion criteria alongside the others following from our PICoST criteria are given in more detail in Table 3 in the formatted text.\nIn the second step,\nfor the relevant documents only, the type of policy in the document will also be labelled. The categories for these labels are outlined in the subsequent section. If a document contains multiple policies, or one policy fits multiple categories, it will also gain multiple labels. If the document does not contain sufficient information to determine if a given label is appropriate, it will be left blank. In practice, these first two steps are done concurrently.\nIn the third step\n,\nthe labelled data from the first two steps is taken to train multiple machine learning algorithms. The documents selected by this algorithm, along with their categories, will form the basis of our analysis.\nA separate, more extensive guide for coders, which also contains additional details on the different categories is available in Supplementary Materials 3. Note that this is a first version; as coding progresses, this guide will be amended with additional information and examples to ensure that the coding guidelines are clear and consistently followed by all.\n4.3.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Consistency & independence\nCoding will be conducted by multiple researchers from different backgrounds. To ensure that their coding is consistent, 15% of the documents will be coded by two or more researchers. This allows us to find disagreements between researchers; such disagreements will be discussed with the wider team until consensus is reached.\nFurthermore, for a conventional systematic review, it is not unusual to address issues of procedural independence \u2013 in practice, mostly ensuring that researchers do not include or exclude work they have themselves authored. Given that the majority of the inclusion/exclusion decisions in this project will be made by an algorithm, this is less of an issue. However, should a researcher encounter their own work during the coding process, they will refrain from coding it.\n4.4\u00a0\u00a0Category definitions\n4.4.1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Translating the NATO typology to adaptation\nAs stated, a major part of our policy categorisation is based on the NATO typology. These four types \u2013 Nodality, Authority, Treasure, Organisation \u2013 form the first level of our categorisation. We also add a more detailed second and third level. This categorization scheme was developed collectively by the research team. See the table in the formatted document given in the supplementary materials.\n4.4.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Maladaptation\nAs adaptation policies are implemented, there is increased attention for the, often unintended, negative consequences of these policies, known as maladaptation. Although there is both a political and scientific debate on the exact meaning and proper use of the term, (Juhola et al., 2016, Glover and Granberg, 2021), we take maladaptation to mean situations where \u201cexposure and sensitivity to climate change impacts are increased as a result of action taken\u201d (Schipper, 2020 p. 409). The maladaptive effects do not need to impact the target group of the policy \u2013 in other words, if an adaptation policy shifts the vulnerability to another group, this is still considered maladaptive. Likewise, policies which negatively impact the general welfare of a population are considered maladaptive, as are actions which increase greenhouse gas emissions.\nWe do not record the type of maladaptation. Instead, we only record if the document provides any evidence of maladaptation or not.\n4.4.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Constraints and limits\nDocuments will also be marked according to whether constraints, limits or any synonyms describing \u201cfactors making it harder to plan and implement adaptation\u201d are mentioned in the abstract. If the answer is \u2018yes\u2019, coders will be asked to specify the type of constraint, choosing from the options in table 5 below. Definitions and categories are based on the AR5 and AR6 (WG2) from the IPCC.\n4.4.4\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Governance Level\nDocuments will be marked according to the level at which the policy is implemented (in theory or in practice), choosing between:\n\u25cf\u00a0\u00a0\u00a0\tInternational (including supranational and regional such as the EU or ASEAN),\n\u25cf\u00a0\u00a0\u00a0\tNational,\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Subnational level (including local, state, province, region, municipal, city).\n4.4.5\u00a0\u00a0\u00a0\u00a0\u00a0Type of impact responded to\nSome documents specify what kind of climate change effect is being responded to. This includes both observed impacts and potential hazards. These are categorised according to IPCC AR5 SPM2 (p. 7), with the addition of the more specific terms drought, heat waves and storms to reflect the increased confidence since that assessment. This results in:\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Glaciers, snow, ice and permafrost\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Rivers, lakes and floods;\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Drought;\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Extreme heat;\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Food production;\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Wildfire;\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Coastal erosion and/or sea level effects;\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Storms and hurricanes;\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Terrestrial ecosystems;\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Marine ecosystems;\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Livelihoods, health or economics.\nIf no specific impact is mentioned, this is left blank. This includes policies which respond to climate change in general.\nIn general, the most specific category will be the one selected (e.g. forest fires are influenced by droughts but will only be classified under Wildfire; agriculture generally depends on terrestrial ecosystems to some extent, but will be classified under Food production). However, the categories are not mutually exclusive, so in case multiple specific impacts are mentioned, all will be recorded.\n4.4.6\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Evidence type\nDocuments will be marked according to whether they provide ex-post or ex-ante evidence on policies. For the purposes of this project, ex-post refers to all studies which analyse the effects of a policy which has already been enacted. Ex-ante refers to all studies which analyse potential effects of a policy once the policy has started being implemented.\n4.4.7\u00a0\u00a0\u00a0\u00a0\u00a0Countries mentioned\nWe will use a pre-trained algorithm (Halterman, 2017) to identify geographic locations in the full search. As this algorithm has been trained on non-academic texts however, countries for the labelled data will be noted so we can estimate the accuracy of this algorithm for our particular dataset.\nWhere documents mention locations or geographical entities, annotators will record the country or countries which contain those geographical entities. For example, if a paper mentions \u201cBerlin\u201d, the annotator will enter \u201cGermany\u201d. Where geographical entities are supranational or non-national (e.g. the European Union, or the Atlantic Ocean) this field shall be left blank.\n4.5\u00a0\u00a0\u00a0Machine learning considerations\n4.5.1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Algorithm\nTo briefly reiterate, in this project we will employ supervised learning of two different types: first, a binary classifier is used for study identification (relevant vs not relevant); on the documents identified as relevant, we will then use a multi-label classifier for each coding level. In both cases, we are using the studies coded by hand as training and validation sets.\nChoosing the appropriate algorithm for both these classification tasks is crucial to ensure that the machine learning predictions are fit for purpose. We will therefore test a variety of models from the Scikit Learn package (Pedregosa et al., 2011). Prior work (Berrang-Ford et al., 2021b, Sietsma et al., 2021) had positive results especially using Support Vector Machines (Chang and Lin, 2011); as this does not natively support multi-label predictions, we will use a one-VS-rest set-up for the category predictions. Following Callaghan et al. (2021)we will also test more state-of-the-art deep learning approaches based on BERT (Devlin et al., 2018), including a BERT model that has undergone additional pre-training for a corpus of documents related to climate change (Webersinke et al., 2021).\nWe will use a nested cross-validation procedure (Cawley and Talbot, 2010, see also Callaghan et al., 2021) to optimize hyperparameters and measure the accuracy of our classifiers. In simple terms, this entails dividing the labelled data up in several subsets. All but one of the subsets are then used to train an algorithm which is used to make predictions on the remaining subset \u2013 this is known as the \u201ctest set\u201d. The procedure is repeated using another subset as the test set until each of the subsets has functioned as test set. If, for example, we divide in ten subsets, this means we have predictions for all labelled documents based on 10 algorithms that were each trained on 90% of the total labelled dataset. Comparing these predictions against the manually created labels provides an estimate of performance with quantified uncertainty. The process can then be repeated with different hyper-parameters and different algorithms. This allows us to choose the algorithm and hyper-parameters that are most appropriate for our dataset.\n4.5.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Random and non-random samples\nEnsuring sufficient numbers of true positives\nmay be a challenge as our initial dataset will be large and relatively unfocussed. With tens of thousands of documents in total and an evidence base that for some of the more specific categories will not be larger than a few dozen documents in total, we will need a mixture of strategies to provide the machine learning algorithm with sufficient examples to learn from, without biasing results. In practice, we will make use of a mixture of the following types of samples:\n\u25cf\nRandom \u2013\nthe majority of the coded documents will be selected at random. These documents will also be used to estimate the performance of our classifier.\n\u25cf\nPreliminary machine learning \u2013\nsome documents will be selected based on preliminary results from our classifier. This serves two purposes: first, to identify early on which areas the classifier is struggling with; second, to increase the number of positive examples.\n\u25cf\nKeyword-based \u2013\nif a particular area is lacking positive examples, some samples will be drawn based on keywords. Care must be taken here not to bias the results, which in practice means choosing keywords that will be used not just by a large majority of the positive examples we are looking to identify, but which are also still used by a substantial body of other literature.\n\u25cf\nFrom literature \u2013\nif there is a need to further increase the number of positive examples, we may choose to create a sample based on the references of key literature (e.g. IPCC reports). This will ensure that the classifier performs well on highly-regarded documents.\nThe degree to which non-random samples will be used is dependent on the performance of the classifier. The type of sample will be recorded for all documents assigned to any given reviewer so that we can ensure a sufficiently large random set is used to evaluate the classifier performance and to prevent bias.\n4.5.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Accuracy targets & size of the training set\nWhen setting accuracy targets for supervised machine learning algorithms, a key consideration is the size of the training set (i.e. the number of hand-coded documents), as this to a large degree determines the performance of the classifier. In theory, it may be possible to set a given level of accuracy (e.g. 95%) and keep increasing the size of the training set until this accuracy is reached. In practice however, using an accuracy target in such a way is often impractical for two reasons: first, hand-coding documents is time-intensive (Haddaway and Westgate, 2019); second, the performance of the classifier cannot increase beyond the quality of the input data. In other words, there is likely to be a substantial grey area on what is considered \u201crelevant\u201d which will lead to inconsistency among coders and therefore inconsistent training data. As a consequence, the algorithm will not be able to accurately distinguish between documents in this grey area either, no matter the size of the training dataset.\nThe extent to which machine learning classifiers can accurately make predictions is unknown\na priori\n. Previous efforts using a similar strategy to the one outlined here have found that, even with thousands of documents coded, the accuracy of the classifier remains lower than what would be ordinarily expected in science, which is to say, less than 90% of true positives (Sietsma et al., 2021, Hsu and Rauber, 2021). Note however that traditional systematic reviews likely also suffer the same issue \u2013 it simply remains unreported as the \u201cperformance\u201d of the human coders there is never quantified. Using the performance metrics obtained through cross-validation, as described earlier, we can provide estimates on the accuracy algorithm and will report these results as well as their implications for uncertainty bands.\nAlthough a simple accuracy target cannot be set, we can use these accuracy scores to estimate when the classifier is reaching its maximum potential. More concretely, for this project, the training dataset will consist of at minimum 1 500 hand-coded documents, of which at least 1000 are from a random sample. This will then be used to train a first version of the classifier, which will be used to estimate if performance is still increasing with increases in the size of the training set. If this is the case, document screening will continue until the increase in accuracy of the classifier has not increased meaningfully for a minimum of 500 documents added.\nThe performance for most categories in the multi-label classifier is likely to be lower than that for the binary inclusion/exclusion classifier, given that the same number of documents is used here to provide information on multiple options within the category. Since we further expect the data to be unbalanced (i.e. some categories will have relatively few positive examples while others have many), we may use additional targeted samples to increase performance of the multi-label classifier. Foregrounded results will be limited to the category types where the classifier achieves consistently high accuracy. Recall also that the NATO-typology is hierarchical, which is useful here: if we do not have enough examples to make positive predictions at a lower level, we may still get usable predictions at the higher level.\n4.6   Data extraction\nOur evidence map is primarily based on the categories predicted for all the relevant documents. The criteria for these categories have been described above, though it is worth repeating that depending on data availability and associated performance of the classifier, some categories may later be merged. As stated also, the geographic location will be based on a pre-trained geoparser, namely Mordecai (Halterman, 2017). The hand-coded locations will only be used to estimate its accuracy.\nMeta data will also be retrieved for all articles. This includes the publication year, allowing for a temporal analysis. Further, the author affiliations often include an address or place name. This field can also be fed through the geoparser to identify the geographic location of the authors.\nIn addition to these categories, we will also make use of topic modelling. This is a so-called \u201cunsupervised\u201d machine learning method. In contrast to the supervised methods described earlier, unsupervised learning does not make use of a training set. Rather, the algorithm searches for structures in the data itself \u2013 in the case of topic modelling specifically, the algorithm will find clusters of words which frequently occur together for a set number of topics. Each topic will then be named by the researchers and topics can be grouped together into overarching topic groups. These topic names and groups will be determined inductively and in combination with the findings from the classifier; as such, we cannot provide additional details on their content before the final dataset of relevant documents has been compiled.\n4.7\u00a0\u00a0\u00a0Data Synthesis\n4.7.1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Synthesis strategy\nOrdinarily, a systematic review or map will result in a narrative summary where vote-counting is especially discouraged (Haddaway et al., 2018). For this machine learning-based project, more quantitative measures however are all but inevitable. Indeed, determining the size of the evidence base for our various categories is among the core objectives for this research and is necessary to provide context to further findings. As such, we expect to start our evidence synthesis with a description of our final dataset and its development over time, as well as its geographic distribution. These basic descriptors may already point towards biases in the evidence base.\nBy combining different layers within the final dataset, we can then investigate more complex questions. For example, the NATO categories can be combined with the results of the topic model to investigate what types of tools are most prevalent in different subject areas. We can also further investigate geographic biases, if there are any, by quantifying the prevalence of different topics per region.\n4.7.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Knowledge gaps and clusters\nSince topic modelling entails the identification of clusters within a document set, this tool is ideally suited to identify knowledge clusters within academic literature. Topics within the topic model can also overlap, which can be used to identify larger topic groups. Moreover, since topic modelling assumes that each document consists of a mixture of topics, we can also investigate the co-occurrence of topics within documents. This can be used to further highlight knowledge clusters \u2013 e.g. if the topic model would include the categories typhoons and re-location, and these topics frequently occur together, this would be an indication that the evidence base here is strong.\nTo identify knowledge gaps, such a table with co-occurrences could also be useful \u2013 e.g. if the same typhoons topic would have little overlap with a coastal zone management topic, this would suggest a lack of evidence. In addition, the categorization of the selected documents should provide some insight into understudied areas \u2013 both in terms of subject areas and in terms of geographic distribution.\nLastly, we can compare our dataset against other current assessments of adaptation science generally and adaptation policy in particular. For this, the upcoming IPCC WG2 assessment as well as the Adaptation Gap Report (United Nations Environment Programme, 2021a) should form a solid basis. In the case of the former, we would for example expect to see more evidence for topics highlighted in the Summary for Policymakers and its figures. Pending government approval, a figure comparing adaptation options is expected.\n4.7.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Critical appraisal\nIt should be noted that we do not control for study quality in any way, except by limiting our search to established databases of peer-reviewed research. This is especially important given the disparate communities of research from which we draw research. These communities have varying epistemological bases and standards, which despite the diversity of researchers involved in this project, the team may not always be equipped to fully appreciate, especially judging from the abstracts alone. Overall, we aim to be inclusive, which may at times result in the inclusion of documents of a scientific standard that would not be acceptable to all. In our view, this is inevitable given the scale of the project.\nThis same scale also means that a small number of papers are unlikely to significantly influence the results. On the one hand, this means that the point raised above about scientific quality only becomes a major concern if the general standard of the field is insufficient; on the other hand, there are indications that in some areas of research, the standard may indeed be low (e.g. Scheelbeek et al., 2021) and voices with more fundamental criticisms may get crowded out. Any narrative emerging from the data should therefore be assessed critically in light of the power structures that underpin the science of adaptation policy (Overland and Sovacool, 2020, Nightingale, 2017). More generally, during the analysis, the whole team should keep in mind that we are not\nevaluating\nadaptation policies, but rather\ndocumenting\nwhere research on adaptation policies is published (similar to the proposal by Tompkins et al., 2018). In short, we assess quantity, not quality of adaptation policy literature. Still, if successful, this would result in the most comprehensive evidence map of adaptation policies to date."
        },
        {
            "header": "Troubleshooting",
            "content": ""
        },
        {
            "header": "Time Taken",
            "content": ""
        },
        {
            "header": "Anticipated Results",
            "content": ""
        },
        {
            "header": "References",
            "content": "<p>Adaptation Committee. Data for adaptation at different spatial and temporal scales. (UNFCCC Secretariat, Bonn, 2020).</p><p>Arnott, J. C., Moser, S. C. &amp; Goodrich, K. A. Evaluation that counts: A review of climate change adaptation indicators &amp; metrics using lessons from effective evaluation and science-practice interaction. <em>Environmental Science &amp; Policy</em> <strong>66</strong>, 383-392, doi:<u>https://doi.org/10.1016/j.envsci.2016.06.017</u> (2016).</p><p>Berrang-Ford, L.<em> et al.</em> A systematic global stocktake of evidence on human adaptation to climate change. <em>Nature Climate Change</em> <strong>11</strong>, 989-1000, doi:10.1038/s41558-021-01170-y (2021).</p><p>Berrang-Ford, L.<em> et al.</em> Mapping global research on climate and health using machine learning (a systematic evidence map) [version 1; peer review: awaiting peer review]. <em>Wellcome Open Research</em> <strong>6</strong>, doi:10.12688/wellcomeopenres.16415.1 (2021).</p><p>Berrang-Ford, L.<em> et al.</em> Systematic mapping of global research on climate and health: a machine learning review. <em>The Lancet Planetary Health</em> <strong>5</strong>, e514-e525, doi:<u>https://doi.org/10.1016/S2542-5196(21)00179-0</u> (2021).</p><p>Berrang-Ford, L., Ford, J. D. &amp; Paterson, J. Are we adapting to climate change? <em>Global Environmental Change</em> <strong>21</strong>, 25-33, doi:<u>https://doi.org/10.1016/j.gloenvcha.2010.09.012</u> (2011).</p><p>Biesbroek, R. &amp; Delaney, A. Mapping the evidence of climate change adaptation policy instruments in Europe. <em>Environmental Research Letters</em> <strong>15</strong>, 083005, doi:10.1088/1748-9326/ab8fd1 (2020).</p><p>Biesbroek, R., Badloe, S. &amp; Athanasiadis, I. N. Machine learning for research on climate change adaptation policy integration: an exploratory UK case study. <em>Regional Environmental Change</em> <strong>20</strong>, 85, doi:10.1007/s10113-020-01677-8 (2020).</p><p>Biesbroek, R., Wright, S. J., Eguren, S. K., Bonotto, A. &amp; Athanasiadis, I. N. Policy attention to climate change impacts, adaptation and vulnerability: a global assessment of National Communications (1994\u20132019). <em>Clim Policy</em> <strong>22</strong>, 97-111, doi:10.1080/14693062.2021.2018986 (2022).</p><p>Callaghan, M.<em> et al.</em> Machine-learning-based evidence and attribution mapping of 100,000 climate impact studies. <em>Nature Climate Change</em> <strong>11</strong>, 966-972, doi:10.1038/s41558-021-01168-6 (2021).</p><p>Callaghan, M. W., Minx, J. C. &amp; Forster, P. M. A topography of climate change research. <em>Nature Climate Change</em> <strong>10</strong>, 118-123, doi:10.1038/s41558-019-0684-5 (2020).</p><p>Capano, G. &amp; Howlett, M. The Knowns and Unknowns of Policy Instrument Analysis: Policy Tools and the Current Research Agenda on Policy Mixes. <em>SAGE Open</em> <strong>10</strong>, 2158244019900568, doi:10.1177/2158244019900568 (2020).</p><p>Cawley, G. C. &amp; Talbot, N. L. On over-fitting in model selection and subsequent selection bias in performance evaluation. <em>The Journal of Machine Learning Research</em> <strong>11</strong>, 2079-2107 (2010).</p><p>Chang, C.-C. &amp; Lin, C.-J. LIBSVM: A library for support vector machines.&nbsp;<strong>2</strong>, Article 27, doi:10.1145/1961189.1961199 (2011).</p><p>Christiansen, L., Olhoff, A. &amp; Dale, T. Understanding adaptation in the Global Stocktake. (UNEP DTU&nbsp;\u2026, Copenhagen, 2020).</p><p>Craft, B. &amp; Fisher, S. Measuring the adaptation goal in the global stocktake of the Paris Agreement. <em>Clim Policy</em> <strong>18</strong>, 1203-1209, doi:10.1080/14693062.2018.1485546 (2018).</p><p>Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em> (2018).</p><p>Ford, J. D. &amp; Berrang-Ford, L. The 4Cs of adaptation tracking: consistency, comparability, comprehensiveness, coherency. <em>Mitigation and Adaptation Strategies for Global Change</em> <strong>21</strong>, 839-859, doi:10.1007/s11027-014-9627-7 (2016).</p><p>Ford, J. D.<em> et al.</em> Big data has big potential for applications to climate change adaptation. <em>P Natl Acad Sci USA</em> <strong>113</strong>, 10729-10732, doi:10.1073/pnas.1614023113 (2016).</p><p>F\u00fcssel, H.-M. &amp; Almond, S. The European Climate Data Explorer-a new web portal providing interactive access to climate change information for Europe. (Copernicus Meetings, 2021).</p><p>Glover, L. &amp; Granberg, M. The Politics of Maladaptation. <em>Climate</em> <strong>9</strong>, 69 (2021).</p><p>Grieneisen, M. L. &amp; Zhang, M. The current status of climate change research. <em>Nature Climate Change</em> <strong>1</strong>, 72-73, doi:10.1038/nclimate1093 (2011).</p><p>Haddaway, N. R. &amp; Westgate, M. J. Predicting the time needed for environmental systematic reviews and systematic maps. <em>Conservation Biology</em> <strong>33</strong>, 434-443, doi:10.1111/cobi.13231 (2019).</p><p>Haddaway, N. R.<em> et al.</em> On the use of computer-assistance to facilitate systematic mapping. <em>Campbell Systematic Reviews</em> <strong>16</strong>, e1129, doi:<u>https://doi.org/10.1002/cl2.1129</u> (2020).</p><p>Haddaway, N. R., Macura, B., Whaley, P. &amp; Pullin, A. S. ROSES RepOrting standards for Systematic Evidence Syntheses: pro forma, flow-diagram and descriptive summary of the plan and conduct of environmental systematic reviews and systematic maps. <em>Environmental Evidence</em> <strong>7</strong>, 7, doi:10.1186/s13750-018-0121-7 (2018).</p><p>Halterman, A. Mordecai: Full Text Geoparsing and Event Geocoding. <em>The Journal of Open Source Software</em> <strong>2</strong>, doi:10.21105/joss.00091 (2017).</p><p>Haunschild, R., Bornmann, L. &amp; Marx, W. Climate Change Research in View of Bibliometrics. <em>PLoS One</em> <strong>11</strong>, e0160393-e0160393, doi:10.1371/journal.pone.0160393 (2016).</p><p>Henstra, D. Climate Adaptation in Canada: Governing a Complex Policy Regime. <em>Review of Policy Research</em> <strong>34</strong>, 378-399, doi:<u>https://doi.org/10.1111/ropr.12236</u> (2017).</p><p>Hood, C. <em>The tools of government</em>.&nbsp;(Springer, 1986).</p><p>Hsu, A. &amp; Rauber, R. Diverse climate actors show limited coordination in a large-scale text analysis of strategy documents. <em>Communications Earth &amp; Environment</em> <strong>2</strong>, 30, doi:10.1038/s43247-021-00098-7 (2021).</p><p>IPCC. Global Warming of 1.5\u00b0C. An IPCC Special Report on the impacts of global warming of 1.5\u00b0C above pre-industrial levels and related global greenhouse gas emission pathways, in the context of strengthening the global response to the threat of climate change, sustainable development, and efforts to eradicate poverty. (IPCC, Geneva, Switzerland, 2018).</p><p>IPCC. Summary for Policymakers., (Oxford, 2021).</p><p>Juhola, S., Glaas, E., Linn\u00e9r, B.-O. &amp; Neset, T.-S. Redefining maladaptation. <em>Environmental Science &amp; Policy</em> <strong>55</strong>, 135-140, doi:<u>https://doi.org/10.1016/j.envsci.2015.09.014</u> (2016).</p><p>Knill, C. &amp; Tosun, J. <em>Public policy: A new introduction</em>.&nbsp;(Bloomsbury Publishing, 2020).</p><p>Konno, K. &amp; Pullin, A. S. Assessing the risk of bias in choice of search sources for environmental meta-analyses. <em>Research Synthesis Methods</em> <strong>11</strong>, 698-713, doi:<u>https://doi.org/10.1002/jrsm.1433</u> (2020).</p><p>Lamb, W. F., Creutzig, F., Callaghan, M. W. &amp; Minx, J. C. Learning about urban climate solutions from case studies. <em>Nature Climate Change</em> <strong>9</strong>, 279-287, doi:10.1038/s41558-019-0440-x (2019).</p><p>Leiter, T. Do governments track the implementation of national climate change adaptation plans? An evidence-based global stocktake of monitoring and evaluation systems. <em>Environmental Science &amp; Policy</em> <strong>125</strong>, 179-188, doi:<u>https://doi.org/10.1016/j.envsci.2021.08.017</u> (2021).</p><p>Lesnikowski, A.<em> et al.</em> Frontiers in data analytics for adaptation research: Topic modeling. <em>Wiley Interdisciplinary Reviews: Climate Change</em> <strong>10</strong>, e576 (2019).</p><p>Lesnikowski, A.<em> et al.</em> What does the Paris Agreement mean for adaptation? <em>Climate Policy</em> <strong>17</strong>, 825-831, doi:10.1080/14693062.2016.1248889 (2017).</p><p>Lesnikowski, A., Ford, J. D., Biesbroek, R. &amp; Berrang-Ford, L. A policy mixes approach to conceptualizing and measuring climate change adaptation policy. <em>Climatic Change</em> <strong>156</strong>, 447-469, doi:10.1007/s10584-019-02533-3 (2019).</p><p>Magnan, A. K. &amp; Chalastani, V. I. Towards a Global Adaptation Progress Tracker: first thoughts. <em>IDDRI</em> <strong>Working Paper N\u00b01</strong> (2019).</p><p>Marshall, I. J. &amp; Wallace, B. C. Toward systematic review automation: a practical guide to using machine learning tools in research synthesis. <em>Systematic reviews</em> <strong>8</strong>, 1-10 (2019).</p><p>Mongeon, P. &amp; Paul-Hus, A. The journal coverage of Web of Science and Scopus: a comparative analysis. <em>Scientometrics</em> <strong>106</strong>, 213-228 (2016).</p><p>Nachmany, M., Byrnes, R. &amp; Surminski, S. Policy brief National laws and policies on climate change adaptation: a global review. (Grantham Research Institute on Climate Change, London, 2019).</p><p>Nakagawa, S.<em> et al.</em> Research weaving: visualizing the future of research synthesis. <em>Trends in ecology &amp; evolution</em> <strong>34</strong>, 224-238 (2019).</p><p>Nightingale, A. J. Power and politics in climate change adaptation efforts: Struggles over authority and recognition in the context of political instability. <em>Geoforum</em> <strong>84</strong>, 11-20, doi:<u>https://doi.org/10.1016/j.geoforum.2017.05.011</u> (2017).</p><p>Nunez-Mir, G. C., Iannone, B. V., Pijanowski, B. C., Kong, N. N. &amp; Fei, S. L. Automated content analysis: addressing the big literature challenge in ecology and evolution. <em>Methods Ecol Evol</em> <strong>7</strong>, 1262-1272, doi:10.1111/2041-210x.12602 (2016).</p><p>Overland, I. &amp; Sovacool, B. K. The misallocation of climate research funding. <em>Energy Research &amp; Social Science</em> <strong>62</strong>, 101349, doi:<u>https://doi.org/10.1016/j.erss.2019.101349</u> (2020).</p><p>Pedregosa, F.<em> et al.</em> Scikit-learn: Machine Learning in Python. <em>Journal of Machine Learning Research</em> <strong>12</strong>, 2825-2830 (2011).</p><p>Persson, \u00c5. &amp; Dzebo, A. Special issue: Exploring global and transnational governance of climate change adaptation. <em>International Environmental Agreements: Politics, Law and Economics</em> <strong>19</strong>, 357-367, doi:10.1007/s10784-019-09440-z (2019).</p><p>Rolnick, D.<em> et al.</em> Tackling Climate Change with Machine Learning. doi:arXiv:1906.05433v2 (2019).</p><p>Runhaar, H., Wilk, B., Persson, \u00c5., Uittenbroek, C. &amp; Wamsler, C. Mainstreaming climate adaptation: taking stock about \u201cwhat works\u201d from empirical research worldwide. <em>Regional Environmental Change</em> <strong>18</strong>, 1201-1210, doi:10.1007/s10113-017-1259-5 (2018).</p><p>Scheelbeek, P. F.<em> et al.</em> The effects on public health of climate change adaptation responses: a systematic review of evidence from low-and middle-income countries. <em>Environ Res Lett</em> <strong>16</strong>, 073001, doi:<u>https://doi.org/10.1088/1748-9326/ac092c</u> (2021).</p><p>Schipper, E. L. F. Maladaptation: When Adaptation to Climate Change Goes Very Wrong. <em>One Earth</em> <strong>3</strong>, 409-414, doi:<u>https://doi.org/10.1016/j.oneear.2020.09.014</u> (2020).</p><p>Sietsma, A. J., Ford, J. D., Callaghan, M. W. &amp; Minx, J. C. Progress in Climate Change Adaptation Research. <em>Environmental Research Letters</em> <strong>In press</strong>, doi:<u>https://doi.org/10.1088/1748-9326/abf7f</u> (2021).</p><p>Tompkins, E. L., Vincent, K., Nicholls, R. J. &amp; Suckall, N. Documenting the state of adaptation for the global stocktake of the Paris Agreement. <em>WIREs Climate Change</em> <strong>9</strong>, e545, doi:10.1002/wcc.545 (2018).</p><p>Ulibarri, N.<em> et al.</em> A global assessment of policy tools to support climate adaptation. <em>Clim Policy</em> <strong>22</strong>, 77-96, doi:10.1080/14693062.2021.2002251 (2022).</p><p>United Nations Environment Programme. Adaptation Gap Report 2021: The gathering storm \u2013 Adapting to climate change in a post-pandemic world. Report No. ISBN: 978-92-807-3834-6, (Nairobi, 2021).</p><p>United Nations Environment Programme. Emissions Gap Report 2020: The Heat Is On \u2013 A World of Climate Promises Not Yet Delivered. (Nairobi, 2021).</p><p>Van de Schoot, R.<em> et al.</em> An open source machine learning framework for efficient and transparent systematic reviews. <em>Nature Machine Intelligence</em> <strong>3</strong>, 125-133 (2021).</p><p>Vera-Baceta, M.-A., Thelwall, M. &amp; Kousha, K. Web of Science and Scopus language coverage. <em>Scientometrics</em> <strong>121</strong>, 1803-1813, doi:10.1007/s11192-019-03264-z (2019).</p><p>Wang, Z., Zhao, Y. &amp; Wang, B. A bibliometric analysis of climate change adaptation based on massive research literature data. <em>Journal of Cleaner Production</em> <strong>199</strong>, 1072-1082, doi:10.1016/j.jclepro.2018.06.183 (2018).</p><p>Webersinke, N., Kraus, M., Bingler, J. A. &amp; Leippold, M. Climatebert: A pretrained language model for climate-related text. <em>arXiv preprint arXiv:2110.12010</em> (2021).</p>"
        },
        {
            "header": "Acknowledgements",
            "content": "<p>The authors declare no competing interests.</p>"
        }
    ],
    "attributes": {
        "acceptedTermsAndConditions": true,
        "allowDirectSubmit": true,
        "archivedVersions": [],
        "articleType": "Method Article",
        "associatedPublications": [],
        "authors": [
            {
                "id": 83196473,
                "identity": "e6736401-00fb-42ee-8e56-0d9b95a3d6d6",
                "order_by": 0,
                "name": "Anne J. Sietsma",
                "email": "eeajs@leeds.ac.uk",
                "orcid": "https://orcid.org/0000-0003-0239-152X",
                "institution": "Priestley International Centre for Climate, University of Leeds, Leeds, United Kingdom",
                "correspondingAuthor": true,
                "prefix": "",
                "firstName": "Anne",
                "middleName": "J.",
                "lastName": "Sietsma",
                "suffix": ""
            },
            {
                "id": 83196474,
                "identity": "c4752192-74ab-4f22-af7c-2d5cb1827ce8",
                "order_by": 1,
                "name": "Max Callaghan",
                "email": "",
                "orcid": "https://orcid.org/0000-0001-8292-8758",
                "institution": "Mercator Research Institute on Global Commons and Climate Change, Berlin, Germany",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Max",
                "middleName": "",
                "lastName": "Callaghan",
                "suffix": ""
            },
            {
                "id": 83196475,
                "identity": "1d639433-f891-44fc-b3af-169b1e5ceb2f",
                "order_by": 2,
                "name": "Robbert Biesbroek",
                "email": "",
                "orcid": "https://orcid.org/0000-0002-2906-1419",
                "institution": "Public Administration and Policy Group, Wageningen University & Research, Wageningen, the Netherlands",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Robbert",
                "middleName": "",
                "lastName": "Biesbroek",
                "suffix": ""
            },
            {
                "id": 83196476,
                "identity": "ce57ee94-0166-42ea-95a5-62b4a69398c7",
                "order_by": 3,
                "name": "Emily Theokritoff",
                "email": "",
                "orcid": "https://orcid.org/0000-0003-0632-9862",
                "institution": "Climate Analytics, Berlin, Germany; Integrative Research Institute on Transformations of Human-Environment Systems, Humboldt University, Berlin, Germany",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Emily",
                "middleName": "",
                "lastName": "Theokritoff",
                "suffix": ""
            },
            {
                "id": 83196477,
                "identity": "d03aa230-af8a-43d4-9cec-5f2fdf32752e",
                "order_by": 4,
                "name": "Adelle Thomas",
                "email": "",
                "orcid": "https://orcid.org/0000-0002-0407-2891",
                "institution": "Climate Analytics, Berlin, Germany; Climate Change Adaptation and Resilience Research Centre, University of The Bahamas, Nassau, Bahamas",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Adelle",
                "middleName": "",
                "lastName": "Thomas",
                "suffix": ""
            },
            {
                "id": 83196478,
                "identity": "84ffcccd-03fd-432c-96b6-b4c368e3f1e4",
                "order_by": 5,
                "name": "Iv\u00e1n Villaverde Canosa",
                "email": "",
                "orcid": "https://orcid.org/0000-0002-9344-6452",
                "institution": ": Priestley International Centre for Climate, University of Leeds, Leeds, United Kingdom",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Iv\u00e1n",
                "middleName": "Villaverde",
                "lastName": "Canosa",
                "suffix": ""
            },
            {
                "id": 83196479,
                "identity": "a63a8ce0-aede-4362-aa64-0108e179d133",
                "order_by": 6,
                "name": "Carl-Friedrich Schleussner",
                "email": "",
                "orcid": "https://orcid.org/0000-0001-8471-848X",
                "institution": "Climate Analytics, Berlin, Germany; Integrative Research Institute on Transformations of Human-Environment Systems, Humboldt University, Berlin, Germany",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Carl-Friedrich",
                "middleName": "",
                "lastName": "Schleussner",
                "suffix": ""
            },
            {
                "id": 83196480,
                "identity": "7a9cc1dc-c746-4e3a-8464-ed77f76cee5c",
                "order_by": 7,
                "name": "James D. Ford",
                "email": "",
                "orcid": "https://orcid.org/0000-0002-2066-3456",
                "institution": "Priestley International Centre for Climate, University of Leeds, Leeds, United Kingdom",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "James",
                "middleName": "D.",
                "lastName": "Ford",
                "suffix": ""
            },
            {
                "id": 83196481,
                "identity": "0d93a6e0-2fe9-44d1-b9e0-8ab53e6e96f6",
                "order_by": 8,
                "name": "Jan C. Minx",
                "email": "",
                "orcid": "https://orcid.org/0000-0002-2862-0178",
                "institution": "Priestley International Centre for Climate, University of Leeds, Leeds, United Kingdom; Mercator Research Institute on Global Commons and Climate Change, Berlin, Germany",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Jan",
                "middleName": "C.",
                "lastName": "Minx",
                "suffix": ""
            }
        ],
        "badges": [],
        "createdAt": "2022-02-11 13:14:52",
        "currentVersionCode": 1,
        "declarations": "",
        "doi": "10.21203/rs.3.pex-1836/v1",
        "doiUrl": "https://doi.org/10.21203/rs.3.pex-1836/v1",
        "draftVersion": [],
        "editorialEvents": [],
        "editorialNote": "",
        "failedWorkflow": [],
        "files": [
            {
                "id": 18608250,
                "identity": "32c11e17-777a-40be-82da-f9a28874f5b9",
                "added_by": "auto",
                "created_at": "2022-02-25 11:52:40",
                "extension": "jpg",
                "order_by": 1,
                "title": "Figure 1",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 34976,
                "visible": true,
                "origin": "",
                "legend": "<p><strong><em>Figure 1</em></strong><em>: overview of the process. Machine learning components are highlighted in red.</em></p>",
                "description": "",
                "filename": "Process.jpg",
                "url": "https://assets.researchsquare.com/files/pex-1836/v1/bfba525b110fd01276128eb2.jpg"
            },
            {
                "id": 18608253,
                "identity": "c5f02394-9ff8-404b-b4bf-66a8c4d18dd4",
                "added_by": "auto",
                "created_at": "2022-02-25 11:55:40",
                "extension": "jpg",
                "order_by": 2,
                "title": "Figure 2",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 1150433,
                "visible": true,
                "origin": "",
                "legend": "<p><strong><em>Table 1: </em></strong><em>the review objective and its key components according to the PICoST framework</em></p>",
                "description": "",
                "filename": "Table1.jpg",
                "url": "https://assets.researchsquare.com/files/pex-1836/v1/18b5e424de9ced323e5f8c3b.jpg"
            },
            {
                "id": 18608164,
                "identity": "c34c976a-f1fa-4ff0-985f-beb041115874",
                "added_by": "auto",
                "created_at": "2022-02-25 11:49:40",
                "extension": "jpg",
                "order_by": 3,
                "title": "Figure 3",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 1945292,
                "visible": true,
                "origin": "",
                "legend": "<p><strong><em>Table 3</em></strong><em>: inclusion/exclusion criteria (papers must meet all to be included)</em></p>",
                "description": "",
                "filename": "table3.jpg",
                "url": "https://assets.researchsquare.com/files/pex-1836/v1/dd494f641d8fc914427a46eb.jpg"
            },
            {
                "id": 18608252,
                "identity": "62990bb7-16b6-47f6-82e7-688361dcae06",
                "added_by": "auto",
                "created_at": "2022-02-25 11:52:40",
                "extension": "jpg",
                "order_by": 4,
                "title": "Figure 4",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 1088710,
                "visible": true,
                "origin": "",
                "legend": "<p><strong><em>Table 5: </em></strong><em>The categories of constraints and limits used for categorizing documents</em></p>",
                "description": "",
                "filename": "Table5.jpg",
                "url": "https://assets.researchsquare.com/files/pex-1836/v1/029513dcf95a210fa833708a.jpg"
            },
            {
                "id": 18608277,
                "identity": "f9b58fa9-8823-444f-b264-493315b6d2fc",
                "added_by": "auto",
                "created_at": "2022-02-25 11:55:47",
                "extension": "pdf",
                "order_by": 0,
                "title": "",
                "display": "",
                "copyAsset": false,
                "role": "manuscript-pdf",
                "size": 554024,
                "visible": true,
                "origin": "",
                "legend": "",
                "description": "",
                "filename": "manuscript.pdf",
                "url": "https://assets.researchsquare.com/files/pex-1836/v1/821732db-57bb-48bd-aae2-a934c9f0b350.pdf"
            },
            {
                "id": 18608161,
                "identity": "39fb9457-8552-4a69-9a07-c0b29a7850ce",
                "added_by": "auto",
                "created_at": "2022-02-25 11:49:40",
                "extension": "pdf",
                "order_by": 1,
                "title": "",
                "display": "",
                "copyAsset": false,
                "role": "supplement",
                "size": 517638,
                "visible": true,
                "origin": "",
                "legend": "<p>Formatted protocol, including all figures and tables</p>",
                "description": "",
                "filename": "220208ClimateChangeAdaptationPolicyProtocolFINALclean.pdf",
                "url": "https://assets.researchsquare.com/files/pex-1836/v1/a4f6ca2a046c5c6523add3bb.pdf"
            },
            {
                "id": 18608162,
                "identity": "0b191c44-93df-47ee-b835-e22db38ad286",
                "added_by": "auto",
                "created_at": "2022-02-25 11:49:40",
                "extension": "pdf",
                "order_by": 2,
                "title": "",
                "display": "",
                "copyAsset": false,
                "role": "supplement",
                "size": 388514,
                "visible": true,
                "origin": "",
                "legend": "<p>SM3: Coder guidelines with detailed screening and tagging criteria</p>",
                "description": "",
                "filename": "InstructionsforCoding.pdf",
                "url": "https://assets.researchsquare.com/files/pex-1836/v1/8d339dff90e7f5887b245841.pdf"
            },
            {
                "id": 18608158,
                "identity": "706bebe7-0ba0-4d26-9542-22a04457d30c",
                "added_by": "auto",
                "created_at": "2022-02-25 11:49:40",
                "extension": "txt",
                "order_by": 3,
                "title": "",
                "display": "",
                "copyAsset": false,
                "role": "supplement",
                "size": 16340,
                "visible": true,
                "origin": "",
                "legend": "<p>SM2: search queries for different databases in plain text format</p>",
                "description": "",
                "filename": "211125Query.txt",
                "url": "https://assets.researchsquare.com/files/pex-1836/v1/7d19d4b0378e1bf1318a922a.txt"
            },
            {
                "id": 18608165,
                "identity": "c25f9ff4-2860-4b1f-ad13-35ce1a2e340f",
                "added_by": "auto",
                "created_at": "2022-02-25 11:49:40",
                "extension": "xlsx",
                "order_by": 4,
                "title": "",
                "display": "",
                "copyAsset": false,
                "role": "supplement",
                "size": 32366,
                "visible": true,
                "origin": "",
                "legend": "<p>SM1: ROSES Systematic Mapping checklist</p>",
                "description": "",
                "filename": "ROSESforSystematicMapProtocols.xlsx",
                "url": "https://assets.researchsquare.com/files/pex-1836/v1/2e1d4a79582427aa15991717.xlsx"
            }
        ],
        "financialInterests": "",
        "fulltextSource": "",
        "fullText": "",
        "funders": [],
        "hasOptedInToPreprint": true,
        "hasPassedJournalQc": "",
        "hideJournal": true,
        "highlight": "",
        "institution": "",
        "isAuthorSuppliedPdf": false,
        "isDeskRejected": "",
        "isHiddenFromSearch": false,
        "isInQc": false,
        "isInWorkflow": false,
        "journal": {
            "display": true,
            "email": "protocol.exchange@nature.com",
            "identity": "protocol-exchange",
            "isNatureJournal": false,
            "hasQc": false,
            "allowDirectSubmit": true,
            "externalIdentity": "",
            "sideBox": "",
            "submissionUrl": "https://protocolexchange.researchsquare.com/submission",
            "title": "Protocol Exchange",
            "twitterHandle": ""
        },
        "keywords": "climate change, adaptation, policy tools, NLP, machine learning, evidence synthesis, evidence map",
        "license": {
            "name": "CC BY 4.0",
            "url": "https://creativecommons.org/licenses/by/4.0/"
        },
        "manuscriptAbstract": "<p><strong>Background</strong> \u2014 Countries around the globe have started implementing policies to respond to the current and future risks of climate change. The scientific literature on these adaptation policies is fragmented and no central typology is generally accepted, making tracking of global adaptation policy progress difficult.</p><p><strong>Methods </strong>\u2014 In this protocol, we describe how we use machine learning methods to classify scientific literature on adaptation policies following the ROSES guidelines. We use a broad search query in Scopus, MEDLINE and Web of Science (up to November 2021). We manually classify a subset of the documents and use this to train multiple supervised machine learning algorithms, including a state-of-the-art algorithm based on BERT. The classification scheme is aimed at providing a multi-functional database: we classify first based on a newly created typology, which is based around the well-established NATO categories of policy instruments; this is supplemented with categories on the types of impacts, evidence on maladaptation, constraints, evidence type, governance level and geographic location.</p><p><strong>Expected results</strong> \u2013 Using the typology and categories, as well as topic modelling, we create an overview of scientific literature on adaptation policies. This describes the breath of policy options, their geographic distribution, developments over time, and under-explored areas. If successful, this would result in the most comprehensive evidence map of adaptation policies to date; building on this, the machine learning algorithms and underlying data can serve as a basis for a living evidence map, moving towards the real-time tracking of adaptation progress.</p>",
        "manuscriptTitle": "Global Tracking of Climate Change Adaptation Policy Using Machine Learning: a Systematic Map Protocol",
        "msid": "",
        "msnumber": "",
        "nonDraftVersions": [
            {
                "code": 1,
                "date": "2022-02-25 11:49:38",
                "doi": "10.21203/rs.3.pex-1836/v1",
                "editorialEvents": [
                    {
                        "type": "communityComments",
                        "content": 0
                    }
                ],
                "status": "published",
                "journal": {
                    "display": true,
                    "email": "info@researchsquare.com",
                    "identity": "researchsquare",
                    "isNatureJournal": false,
                    "hasQc": true,
                    "allowDirectSubmit": true,
                    "externalIdentity": "",
                    "sideBox": "",
                    "submissionUrl": "/submission",
                    "title": "Research Square",
                    "twitterHandle": "researchsquare"
                }
            }
        ],
        "origin": "",
        "ownerIdentity": "3fcc70a6-d13b-4f51-9e0f-8b85bbe2d300",
        "owner": [],
        "postedDate": "February 25th, 2022",
        "published": true,
        "revision": "",
        "status": "posted",
        "subjectAreas": [
            {
                "id": 10361198,
                "name": "Climate science"
            },
            {
                "id": 10361199,
                "name": "Information theory and computation"
            }
        ],
        "tags": [],
        "versionOfRecord": [],
        "versionCreatedAt": "2022-02-25 11:49:38",
        "video": "",
        "vorDoi": "",
        "vorDoiUrl": "",
        "workflowStages": []
    }
}