{
    "identity": "nprot-6171",
    "title": "Protocol for Convolutional Neural Networks based Automated Cellular Cryo-Electron Tomograms Annotation",
    "content": [
        {
            "header": "Introduction",
            "content": "For optimal preservation, CryoET is preferred over staining and plastic embedding techniques, but the resulting tomograms have very low contrast and suffer from the missing wedge artifact common to TEM tomography experiments.  Interpreting the resulting tomograms requires feature identification and annotation of the features present in the volume. This process may then be followed by additional processing, ranging from measurement, counting and statistical analysis to subtomogram averaging of specific components, providing true _in-situ_ macromolecular structure. Unfortunately, the annotation process remains the rate-limiting step. While specialized tools have been developed for localization of specific features, due to the high noise levels, artifacts, and wide variation among different cell types, general-purpose annotation remains an almost entirely manual task. \r\n\n\n\r\n\n\nHere we introduce the use of deep convolutional neural networks \\(CNN) to produce a system which can mimic a human annotator, and produce annotations of comparable quality. While human annotation and assessment is still required, we this process can dramatically reduce the per-tomogram human effort."
        },
        {
            "header": "Equipment",
            "content": "**Hardware**\r\n\n\nThe CNN based tomogram annotation protocol works best on a workstation with a CUDA compatible GPU. While the software is still able to function on a single core of a CPU at a gratly reduced speed, the time required makes this impractical. The GPU needs to be equipped with sufficient memory to store the training set as well as the CNN. This is roughly 1 GB with default parameters but could be larger in some situations. Ideally, a dedicated GPU should be used for computation with a second, generally lower end, GPU for display. \r\n\n\n\r\n\n\n**Software installation**\r\n\n\nThe software is distributed with \"EMAN2\":http://eman2.org. Given the effective requirement of CUDA, at present, Linux is the only recommended platform, though emerging support for OpenCL means that accelerated support for the Mac may be possible in the near future.\r\n\n\nSince the setup of GPU computing environment closely depends on the hardware and operating system, this is an independent process from installation of EMAN2. Instructions for installing current versions of CUDA change frequently, but can be found on the \"Nvidia website\":https://developer.nvidia.com/cuda-toolkit. \r\n\n\nThe CNN implementation in this software is based on the Python library Theano. Although Theano is already installed during EMAN2 installation, it still needs to be configured to use the correct GPU environment after CUDA setup. A guide for GPU usage in Theano can be found \"here\":http://deeplearning.net/software/theano/tutorial/using_gpu.html.\r\n\n\nDue to the rapid development of GPU and deep learning techniques, the related software packages are under constant development and the protocol described in this paper is subject to change. We provide up to date tutorials \"here\":http://eman2.org/Tutorials."
        },
        {
            "header": "Procedure",
            "content": "**Setup workspace**\r\n\n\nFirst, make an empty folder and cd into that folder at the command-line. Run e2projectmanager.py. While a GUI window will appear, messages may appear in the terminal used to launch the program, so it is useful to keep this window visible.\r\n\n\nClick the **Workflow Mode** drop-down menu next to navigate to the TomoSeg panel.\nSee figure in Figures section.\n**Import Tomograms**\r\n\n\nClick **Import Tomogram Files** on the left panel \\(1). On the panel showed up on the right, click **Browse** next to import_files \\(2), select the tomogram you would like to segment in the browser window, and click **Ok**. If you want to downsample/bin the tomogram before processing, write the shrink factor in appropriate box \\(3). For example, you might wish to reduce a 4k x 4k tomogram to 1k x 1k for efficiency. In this case you would enter 4 in the text box. Also make sure that the **import_tomos** and **tomoseg_auto** box is checked. Finally, click **Launch** \\(4) and wait for pre-processing to finish.\nSee figure in Figures section.\n**Select Positive Examples**\r\n\n\nSelect **Box training references**. Press browse, and select an imported tomogram. Leave **boxsize** as -1, and press Launch. \r\n\n\nIn a moment, three windows will appear: the tomogram view, the selected particles view, and a control-panel. Unlike the 2-D particle picking interface used in single particle analysis, which is almost identical, this program allows you to move through the various slices of the tomogram. The examples you select will be 2-D, drawn from arbitrary slices throughout the 3-D tomogram. \r\n\n\nIn the window named **e2boxer**, make sure your box size is 64. None of the other options need to be changed. \r\n\n\nIn the window containing your tomogram, you can begin selecting boxes. You can move through the slices of the tomogram with the up and down arrows, and zoom in and out with the scroll wheel. Alternatively, you can middle-click on the image to open a control-panel containing sliders with similar functionality. Select and reposition regions of interest \\(ROIs) with the left mouse button. Hold down shift when clicking to delete existing ROIs. As you select regions, they will appear in the \\(Particles) window. \r\n\n\nSelect around 10 ROIs containing the feature you wish to annotate. You will repeat this training process from scratch for different features, so for now, focus just on a single feature type. If the same feature appears different in different layers or different regions of the cell, be sure to cover the full range of visual differences in the representative regions you select. \r\n\n\nWhen selecting boxes, ensure that feature is clear in the \\(Particles) window. You will manually identify the feature in the next step, so it is critical that you can accurately identify the feature throughout each ROI. It is better to have fewer ROIs that you can annotate well than more ROIs with ambiguous annotations.\nSee figure in Figures section.\nAfter identifying an appropriate number of boxes, press **Write output** in the e2boxer window. \r\n\n\n1. Select your tomograms in the **Raw Data** window. \r\n  2. Select a suffix for the ROIs in the **Output Suffix** text box \\(perhaps _good). \r\n  3. In **Normalize Images**, select None. \r\n  4. Press **OK**.\nSee figure in Figures section.\n**Manually Annotate Samples**\r\n\n\nThe next step is to manually identify the feature within each ROI.\r\n\n\nNavigate to the **Segment training references** interface in the project manager window. For **Particles**, browse to the output ROIs you generated in the previous step. \r\n\n\nLeave **Output** blank, keep segment checked, and press **Launch**. \r\n\n\nTwo windows will appear, one containing images, and the other a control panel. You can navigate through the images similarly to looking through the slices of the tomogram above. The control panel will open with the **Draw** tab selected. Using the left mouse button, draw on each of the ROIs to exactly cover the feature of interest as best you can. If necessary you can always can go back to the ROI selection window and check the surroundings of each region to aid in segmentation.\r\n\n\nSegment all of your ROIs. If you need to change the size of the pen, change both **Pen Size** and **Pen Size2** to a larger or smaller number. You would like the marked region to match the feature as closely as possible, so reduce the pen size if it is larger than the feature. \r\n\n\nWhen you are finished, simply close the windows. The segmentation file will be saved automatically as \"*_seg.hdf\" with the same base file name as your ROIs.\nSee figure in Figures section.\n**Select Negative Samples**\r\n\n\nNext you need to identify regions in the tomogram which do not contain the feature of interest at all. Return to the same interface you used to select positive examples, and press the **Clear** button in the **e2boxer** window. This deletes all of your previous selections \\(the positive examples), so make sure you have finished the previous steps before doing this. \r\n\n\nIn the tomogram window, select boxes that DON\u2019T contain the feature of interest. You can select as many of these as you like \\(normally ~100). Try to include a wide variety of different non-feature cellular structures, empty space, gold fiducials and high-contrast carbon. \r\n\n\nAfter finishing picking the negative samples, write the particle output in same way you did for the positive samples, but use a different **Output Suffix**, like \"_bad\". \r\n\n\n\r\n\n\n**Build Training Set**\r\n\n\n1. Select the **Build training set** option in the project manager. \r\n  2. In **particles_raw**, select your \"_good\" file. \r\n  3. In **particles_label**, select your \"_good_seg\" file. \r\n  4. In **boxes_negative**, select your \"_bad\" file. \r\n  Leave **trainset_output blank**. Ncopy controls the number of particles in your training set. The default of 10 is fine, unless you want a faster run at the expense of accuracy. \r\n\n\nPress **Launch**. The program will print \"Done\" in your Terminal when it has finished. The training set will be saved with the same name as the positive particles with \"_trainset\" suffix. \r\n\n\n\r\n\n\n**Train Neural Network**\r\n\n\nOpen **Train the neural network** in the project manager. In **trainset**, browse and choose the \"_trainset\" file. The defaults for everything else in this window are sufficient to produce good results.  To significantly shorten the length of the training \\(but potentially reduce the quality), reduce the number of iterations. Write the filename of the trained neural network output in the **netout** text box, and leave the **from_trained** box empty if it is the first training process. Note that if you have not configured Theano to use CUDA as described above, this will run on a single CPU and take a VERY long time.\r\n\n\nPress **Launch**. The program will print a few numbers quickly at the beginning \\(this is to monitor the training process. Something is wrong if it prints really huge values or takes more than a few seconds to print a number), and then will notify you once it's completed each iteration. When it's finished, it will output the trained neural network in the specified **netout** file and samples of the training result in a file called \"trainout_\" followed by the netout file name. \r\n\n\nAfter the training is finished, it is recommended to look at the training result before proceeding. Open the \"trainout_*.hdf\" file from the e2display window \\(use show stack), and you should see something like this.\nSee figure in Figures section.\nZoom in or out a bit so there are 3*N images displayed in each row. For each three images, the first one is the ROI from the tomogram, the second is the corresponding manual segmentation, and the third is the neural network output for the same ROI after training. The neural network is considered well trained if the third image matches the second image. For the negative particles, both the second and the third images should be largely blank \\(the 3rd may contain some very weak features, this is ok). \r\n\n\nIf the training result looks somewhat wrong, go back and check your positive and negative training set first. Most significant errors are caused by training set errors, i.e. having some positive particles in the negative training set, or one of the positive training set is not correctly segmented. If the training result looks suboptimal \\(the segmentation is not clear enough but not totally wrong), you may consider continuing the training for a few rounds. To do this, go back to the **Train the neural network** panel, choose the previously trained network for the **from_trained** box and launch the training again. It is usually better to set a smaller learning rate in the continued training. Consider change value in the learnrate to ~0.001, or the displayed learning rate value at the last iteration of the previous training. \r\n\n\nIf you are satisfied with the result, go to the next step to segment the whole tomogram. \r\n\n\n\r\n\n\n**Apply to Tomograms**\r\n\n\nFinally, open **Apply the neural network** panel. Choose the tomogram you used to generate the boxes in the tomograms box, choose the saved neural network file \\(not the \"trainout_\" file, which is only used for visualization), and set the output filename. You can change the number of threads to use by adjusting the **thread** option. Keep in mind that using more threads will consume more memory as the tomogram slices are read in at the same time. For example, processing a 1k x 1k x 512 downsampled tomogram on 10 cores would use\n5 GB of RAM. Processing an unscaled 4k x 4k x 1k tomogram would increase RAM usage to\n24 GB. When this process finishes, you can open the output file in your favourite visualization software to view the segmentation.\nSee figure in Figures section.\nTo segment a different feature, just repeat the entire process for the each feature of interest. Make sure to use different file names \\(eg - _good2 and _bad2)\\! The trained network should generally work well on other tomograms using a similar specimen with similar microscope settings \\(clearly the A/pix value must be the same).\r\n\n\n\r\n\n\n**Merging multiple annotation results**\r\n\n\nMerging the results from multiple networks on a single tomogram can help resolve ambiguities, or correct regions which were apparently misassigned. For example, in the microtubule annotation shown above, the carbon edge is falsely recognized as a microtubule. An extra neural network can be trained to  specifically recognize the carbon edge and its result can be competed against the microtubule annotation. A multi-level mask is produced after merging multiple annotation result in which the integer values in a voxel identify the type of feature the voxel contains. To merge multiple annotation results, simply run in the terminal:\r\n\n\nmergetomoseg.py **annotation #1** **annotation #2** ... --output **output mask file**\r\n\n\n\r\n\n\n**Tips in selecting training samples**\r\n\n\n\u2022 Annotate samples correctly, as a bad segmentation in the training set can damage the overall performance. In the microtubule case, if you annotate the spacing between microtubules, instead of the microtubules themselves \\(it is actually quite easy to make such mistake when annotating microtubule bundles), the neural network can behave unpredictably and sometimes just refuse to segment anything. Here is the training result on an incorrect and correct segmentation in one training set. Note the top one \\(22) is annotating the space between microtubules.\nSee figure in Figures section.\n\u2022 Make sure there are no positive samples in the negative training set. If your target feature is everywhere and it is hard to find negative regions, you can add additional positive samples which include various features other than the target feature \\(annotating only the target feature). \r\n\n\n\u2022 You can bin your tomogram differently to segment different features. Just import multiple copies of raw tomogram with different shrink factors, and unbin the segmentation using math.fft.resample processor. It is particularly useful when you have features with different lengthscales in one tomogram, and it is impossible to both fit the large features into a 64*64 box and still have the smaller features visible at the same scale. \r\n\n\n\u2022 In some cases, there is a significant missing wedge in the x-y plane slices \\(you can visualize this by clicking Amp button when looking at the slices in EMAN2). So the resolvability on x direction is different than that on y direction. It is important to provide features running in different directions in the training set, otherwise the neural net may only pick up features in one direction based on the Fourier patten. Also, you may want to check the stage of the microscope, since this may suggest the sample is not tilted exactly around the x axis. \r\n\n\n\u2022 It is also vital to cover various states of the target feature. For example, if you want to segment single layer membranes, you may want to have some cell membrane, some small vesicles, and some vesicles with darker density inside, so the neural network can grab the concept of membrane. Just imagine how you would teach someone with no biological knowledge about the features you are looking for. On the other hand, it is possible to ask the neural network to separate different types of those same features. In the membrane example, it is possible to train the neural network to segment vesicles from cell membranes based on the curvature, or recognize dense vesicles based on the difference of intensity on both side of the membrane, given carefully picked training set."
        },
        {
            "header": "Timing",
            "content": "Training of the CNN takes <5min on a current generation GPU. Application of the CNN on a 4096x4096x1 slice takes ~0.24 CPU hours, and the time taken scale linearly with the number of voxels in the tomogram."
        }
    ],
    "attributes": {
        "acceptedTermsAndConditions": true,
        "allowDirectSubmit": true,
        "archivedVersions": [],
        "articleType": "Method Article",
        "associatedPublications": [
            {
                "doi": "10.1038/nmeth.4405",
                "date": "2017-08-29 15:13:23",
                "title": "Convolutional neural networks for automated annotation of cellular cryo-electron tomograms",
                "authors": [
                    "Muyuan Chen",
                    "Wei Dai",
                    "Stella Y Sun",
                    "Darius Jonasch",
                    "Cynthia Y He",
                    "Michael F Schmid",
                    "Wah Chiu",
                    "and Steven J Ludtke"
                ],
                "journal": "Nature Methods",
                "logo": ""
            }
        ],
        "authors": [
            {
                "id": 15010,
                "identity": "a122fad8-516f-11e9-9e20-12b504df345a",
                "order_by": 0,
                "name": "Steven J. Ludtke",
                "email": "sludtke@bcm.edu",
                "orcid": "",
                "institution": "Ludtke Lab, Baylor College of Medicine",
                "correspondingAuthor": true,
                "prefix": "",
                "firstName": "Steven",
                "middleName": "J.",
                "lastName": "Ludtke",
                "suffix": ""
            },
            {
                "id": 15003,
                "identity": "a122ed5f-516f-11e9-9e20-12b504df345a",
                "order_by": 1,
                "name": "Muyuan Chen",
                "email": "",
                "orcid": "",
                "institution": "Ludtke Lab, Baylor College of Medicine",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Muyuan",
                "middleName": "",
                "lastName": "Chen",
                "suffix": ""
            },
            {
                "id": 15004,
                "identity": "a122f021-516f-11e9-9e20-12b504df345a",
                "order_by": 1,
                "name": "Dai Wei",
                "email": "",
                "orcid": "",
                "institution": "Department of Cell Biology and Neuroscience, Center for Integrative Proteomics Research, Rutgers University, Piscataway, New Jersey USA",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Dai",
                "middleName": "",
                "lastName": "Wei",
                "suffix": ""
            },
            {
                "id": 15005,
                "identity": "a122f1e9-516f-11e9-9e20-12b504df345a",
                "order_by": 1,
                "name": "Stella Y. Sun",
                "email": "",
                "orcid": "",
                "institution": "Verna Marrs and McLean Department of Biochemistry and Molecular Biology, Baylor College of Medicine, Houston, Texas, USA",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Stella",
                "middleName": "Y.",
                "lastName": "Sun",
                "suffix": ""
            },
            {
                "id": 15006,
                "identity": "a122f493-516f-11e9-9e20-12b504df345a",
                "order_by": 1,
                "name": "Darius Jonasch",
                "email": "",
                "orcid": "",
                "institution": "Verna Marrs and McLean Department of Biochemistry and Molecular Biology, Baylor College of Medicine, Houston, Texas, USA",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Darius",
                "middleName": "",
                "lastName": "Jonasch",
                "suffix": ""
            },
            {
                "id": 15007,
                "identity": "a122f655-516f-11e9-9e20-12b504df345a",
                "order_by": 1,
                "name": "Cynthia Y. He",
                "email": "",
                "orcid": "",
                "institution": "Department of Biological Science, Centre for BioImaging Sciences, National University of Singapore, Singapore ",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Cynthia",
                "middleName": "Y.",
                "lastName": "He",
                "suffix": ""
            },
            {
                "id": 15008,
                "identity": "a122f7d0-516f-11e9-9e20-12b504df345a",
                "order_by": 1,
                "name": "Michael F. Schmid",
                "email": "",
                "orcid": "",
                "institution": "Verna Marrs and McLean Department of Biochemistry and Molecular Biology, Baylor College of Medicine, Houston, Texas, USA",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Michael",
                "middleName": "F.",
                "lastName": "Schmid",
                "suffix": ""
            },
            {
                "id": 15009,
                "identity": "a122f961-516f-11e9-9e20-12b504df345a",
                "order_by": 1,
                "name": "Wah Chiu",
                "email": "",
                "orcid": "",
                "institution": "Verna Marrs and McLean Department of Biochemistry and Molecular Biology, Baylor College of Medicine, Houston, Texas, USA",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Wah",
                "middleName": "",
                "lastName": "Chiu",
                "suffix": ""
            }
        ],
        "badges": [],
        "createdAt": "2017-08-03 18:52:27",
        "currentVersionCode": 1,
        "declarations": "",
        "doi": "10.1038/protex.2017.095",
        "doiUrl": "https://doi.org/10.1038/protex.2017.095",
        "draftVersion": [],
        "editorialEvents": [],
        "editorialNote": "",
        "failedWorkflow": [],
        "files": [
            {
                "id": 2609879,
                "identity": "4b51cba2-4f69-4845-9c2a-757277dc0f39",
                "added_by": "29b0f50f-991e-4cf6-96fc-2c41c75f34f2",
                "created_at": "2020-09-25 20:51:15",
                "extension": "png",
                "order_by": 1,
                "title": "Figure 1",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 52755,
                "visible": true,
                "origin": "",
                "legend": "Fig1   e2projectmanager",
                "description": "",
                "filename": "figure1.png",
                "url": "https://assets.researchsquare.com/files/nprot-6171/v1/figure_1.png"
            },
            {
                "id": 2609876,
                "identity": "96000215-87eb-4264-90b7-4200dc759daf",
                "added_by": "29b0f50f-991e-4cf6-96fc-2c41c75f34f2",
                "created_at": "2020-09-25 20:51:15",
                "extension": "png",
                "order_by": 2,
                "title": "Figure 2",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 41090,
                "visible": true,
                "origin": "",
                "legend": "Fig2   Import tomogram",
                "description": "",
                "filename": "figure2.png",
                "url": "https://assets.researchsquare.com/files/nprot-6171/v1/figure_2.png"
            },
            {
                "id": 2609880,
                "identity": "e52f5f44-d31f-4326-9513-890e0ea9b386",
                "added_by": "29b0f50f-991e-4cf6-96fc-2c41c75f34f2",
                "created_at": "2020-09-25 20:51:15",
                "extension": "png",
                "order_by": 3,
                "title": "Figure 3",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 1009546,
                "visible": true,
                "origin": "",
                "legend": "Fig3   Choose training samples",
                "description": "",
                "filename": "figure3.png",
                "url": "https://assets.researchsquare.com/files/nprot-6171/v1/figure_3.png"
            },
            {
                "id": 2609878,
                "identity": "f2a44976-b3af-4dc9-8c69-0802200b2cbb",
                "added_by": "29b0f50f-991e-4cf6-96fc-2c41c75f34f2",
                "created_at": "2020-09-25 20:51:15",
                "extension": "png",
                "order_by": 4,
                "title": "Figure 4",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 35703,
                "visible": true,
                "origin": "",
                "legend": "Fig4   Generate particles",
                "description": "",
                "filename": "figure4.png",
                "url": "https://assets.researchsquare.com/files/nprot-6171/v1/figure_4.png"
            },
            {
                "id": 2609877,
                "identity": "83712575-900d-4935-954b-04ab3dc447b1",
                "added_by": "29b0f50f-991e-4cf6-96fc-2c41c75f34f2",
                "created_at": "2020-09-25 20:51:15",
                "extension": "png",
                "order_by": 5,
                "title": "Figure 5",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 47874,
                "visible": true,
                "origin": "",
                "legend": "Fig5   Annotate samples",
                "description": "",
                "filename": "figure5.png",
                "url": "https://assets.researchsquare.com/files/nprot-6171/v1/figure_5.png"
            },
            {
                "id": 2609882,
                "identity": "4d90ed13-de84-4060-9f2a-abf39db8bfb6",
                "added_by": "29b0f50f-991e-4cf6-96fc-2c41c75f34f2",
                "created_at": "2020-09-25 20:51:15",
                "extension": "png",
                "order_by": 6,
                "title": "Figure 6",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 181281,
                "visible": true,
                "origin": "",
                "legend": "Fig6   Training output",
                "description": "",
                "filename": "figure6.png",
                "url": "https://assets.researchsquare.com/files/nprot-6171/v1/figure_6.png"
            },
            {
                "id": 2609883,
                "identity": "733c7407-c9c1-4d6b-85f3-cebe6ab1a23b",
                "added_by": "29b0f50f-991e-4cf6-96fc-2c41c75f34f2",
                "created_at": "2020-09-25 20:51:15",
                "extension": "png",
                "order_by": 7,
                "title": "Figure 7",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 289710,
                "visible": true,
                "origin": "",
                "legend": "Fig7   Annotation output",
                "description": "",
                "filename": "figure7.png",
                "url": "https://assets.researchsquare.com/files/nprot-6171/v1/figure_7.png"
            },
            {
                "id": 2609881,
                "identity": "a80cbadc-9208-4b5e-9326-dbed7bb08892",
                "added_by": "29b0f50f-991e-4cf6-96fc-2c41c75f34f2",
                "created_at": "2020-09-25 20:51:15",
                "extension": "png",
                "order_by": 8,
                "title": "Figure 8",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 19342,
                "visible": true,
                "origin": "",
                "legend": "Fig8   Bad annotation",
                "description": "",
                "filename": "figure8.png",
                "url": "https://assets.researchsquare.com/files/nprot-6171/v1/figure_8.png"
            },
            {
                "id": 13465221,
                "identity": "ae025382-a5c6-4998-bb61-1b8a5e85665a",
                "added_by": "auto",
                "created_at": "2021-09-16 20:46:00",
                "extension": "pdf",
                "order_by": 0,
                "title": "",
                "display": "",
                "copyAsset": false,
                "role": "manuscript-pdf",
                "size": 1438917,
                "visible": true,
                "origin": "",
                "legend": "",
                "description": "",
                "filename": "manuscript.pdf",
                "url": "https://assets.researchsquare.com/files/nprot-6171/v1/52b15d05-8323-4925-996e-e96248cbd0ef.pdf"
            }
        ],
        "financialInterests": "The authors declare no competing financial interests.",
        "fulltextSource": "",
        "fullText": "",
        "funders": [],
        "hasOptedInToPreprint": true,
        "hasPassedJournalQc": "",
        "hideJournal": true,
        "highlight": "",
        "institution": "",
        "isAuthorSuppliedPdf": false,
        "isDeskRejected": "",
        "isHiddenFromSearch": false,
        "isInQc": false,
        "isInWorkflow": false,
        "journal": {
            "display": true,
            "email": "protocol.exchange@nature.com",
            "identity": "protocol-exchange",
            "isNatureJournal": false,
            "hasQc": false,
            "allowDirectSubmit": true,
            "externalIdentity": "",
            "sideBox": "",
            "submissionUrl": "https://protocolexchange.researchsquare.com/submission",
            "title": "Protocol Exchange",
            "twitterHandle": ""
        },
        "keywords": "Cellular Electron Cryotomography, convolutional neural network, tomogram annotation",
        "license": {
            "name": "CC BY 4.0",
            "url": "https://creativecommons.org/licenses/by/4.0/"
        },
        "manuscriptAbstract": "Cellular Electron Cryotomography \\(CryoET) offers the ability to look inside cells and observe macromolecules frozen in action. A primary challenge for this technique is identifying and extracting the molecular components within the crowded cellular environment. We introduce a method using convolutional neural networks to dramatically reduce the time and human effort required for subcellular annotation and feature extraction.",
        "manuscriptTitle": "Protocol for Convolutional Neural Networks based Automated Cellular Cryo-Electron Tomograms Annotation",
        "msid": "",
        "msnumber": "",
        "nonDraftVersions": [
            {
                "code": 1,
                "date": "2017-08-29 15:15:36",
                "doi": "10.1038/protex.2017.095",
                "editorialEvents": [
                    {
                        "type": "communityComments",
                        "content": 0
                    }
                ],
                "status": "published",
                "journal": {
                    "display": true,
                    "email": "info@researchsquare.com",
                    "identity": "researchsquare",
                    "isNatureJournal": false,
                    "hasQc": true,
                    "allowDirectSubmit": true,
                    "externalIdentity": "",
                    "sideBox": "",
                    "submissionUrl": "/submission",
                    "title": "Research Square",
                    "twitterHandle": "researchsquare"
                }
            }
        ],
        "origin": "",
        "ownerIdentity": "a122956e-516f-11e9-9e20-12b504df345a",
        "owner": [],
        "postedDate": "August 29th, 2017",
        "published": true,
        "revision": "",
        "status": "posted",
        "subjectAreas": [
            {
                "id": 4020,
                "name": "Computational biology and bioinformatics"
            },
            {
                "id": 4021,
                "name": "Biological techniques"
            },
            {
                "id": 4022,
                "name": "Mathematics and computing"
            }
        ],
        "tags": [],
        "versionOfRecord": [],
        "versionCreatedAt": "2017-08-29 15:15:36",
        "video": "",
        "vorDoi": "",
        "vorDoiUrl": "",
        "workflowStages": []
    }
}