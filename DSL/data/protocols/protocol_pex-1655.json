{
    "identity": "pex-1655",
    "title": "<p>Human and machine learning pipelines for responsible clinical prediction using high-dimensional data</p>",
    "content": [
        {
            "header": "Introduction",
            "content": "<p>Several health outcomes have been predicted by machine learning algorithms with satisfying performances for both categorical and numerical outcomes.<sup>1-4</sup> Recently, the most of well-known successes are deep-learning models that surpass human-level performance for diagnostic tasks.<sup>5-7</sup> However, prognostication with causal reasoning is warranted for prevention purpose. Machine learning can learn data distribution for prognostication but cannot infer causality yet, which is what may happen if the distribution differs from those existing in the dataset. But, human learning can learn causality, although our comprehension on big data is limited.<sup>8</sup> Therefore, application of both human and machine learning are inevitable to achieve responsible clinical prediction. We proposed an analysis pipeline using several protocols previously described elsewhere.<sup>9-12</sup></p><p>\tThe first building block of this pipeline is a systematic human learning algorithm.<sup>9</sup> Based on hypothetico-deductive reasoning, human learning involves collection of prior knowledge to construct a causal diagram as a central assumption for hypothesis testing. Subsequently, statistical methods are used to verify the assumption. Solely using statistical analysis on available data without contextual knowledge can introduce data-driven bias. However, humans need machines to deal with enormous amounts of data. Causal diagram is expected to systematically mitigate this type of bias when a human interprets pattern from machine learning results.<sup>13</sup></p><p>\tThe second building block of this pipeline is optional for those using electronic medical records. We provide data extraction protocol for medical histories using Kaplan-Meier estimators, so called historical rate.<sup>10</sup> These are sensible quantities to be differential through time for affecting a future health state. This protocol also allows comparable medical history across healthcare providers, since this type of data is often isolated within each healthcare provider. By population-level historical rate, it is possible to utilize the isolated data across healthcare providers without accessing the respective databases.</p><p>\tThe third building block of this pipeline is to deal with high-dimensional data. We proposed a resampling protocol for dimensional reduction resulting a few latent variables.<sup>11</sup> Most prediction models that used latent variables, if not all, conducted a dimensional reduction without either resampling or data partition, which exposed to a risk of optimistic bias, and is not robust for samples beyond the training set. This is because resampling or data partition are more well-known in either predictive modeling or supervised machine learning, compared to a dimensional reduction that is typically used for statistical inference and unsupervised machine learning.</p><p>\tThe fourth building block of this pipeline is predictive modeling by machine learning. For machine learning, we applied competition winning, state-of-the-art algorithms for tabular data, which were random forest (RF) and gradient boosting machine (GBM). For prognostication, these algorithms were found to outperform others for similar outcomes.<sup>1</sup> We also developed a deep-insight visible neural network (DI-VNN) protocol,<sup>12</sup> according to recent studies.<sup>14,15</sup> But, unlike any of those studies, our protocol allows a human to deeply explore the \u2018subconscious mind\u2019 of the machine learning prediction model to gain insights and to identify bias exploited for predictions. </p><p>\tWe applied this pipeline to several studies which were parts of a DI-VNN project. These studies applied our DI-VNN algorithm to a variety of predicted outcomes with comparison to those applying human learning with statistical methods and predictive modeling with other machine learning algorithms. Ethical clearance was waived by the Taipei Medical University Joint Institutional Review Board (TMU-JIRB number: N202106025). We followed guidelines for developing and reporting machine learning predictive models in biomedical research.<sup>16</sup> We also conducted a self-assessment following the PROBAST guidelines.<sup>17</sup> Those guidelines are specific to multivariable prediction models for making individualized, prognostic, or diagnostic predictions, instead of those applying multivariable modeling to identify risk or prognostic factors.<sup>17</sup> To ensure the clinical suitability of our models, we also fulfilled a clinician checklist for assessing the suitability of machine learning applications in healthcare.<sup>18</sup> We also followed other guidelines to find comparable models to evaluate success criteria.<sup>19</sup> This protocol aimed to provide several building blocks that can be applied either completely or partially to develop and validate clinical prediction using high dimensional data by both human and machine learning.</p>"
        },
        {
            "header": "Reagents",
            "content": ""
        },
        {
            "header": "Equipment",
            "content": "<p>We used R 4.0.2 programming language (R Foundation, Vienna, Austria) to conduct most steps of the data analysis. For steps related to the DI-VNN, we also used Python 3.6.3 programming language (Anaconda Inc., Austin, TX, USA). The integrated development environment software was RStudio 1.3.959 (RStudio PBC, Boston, MA, USA). To ensure reproducibility, we used Bioconductor 3.11;<sup>20</sup> thus, versions of the included R packages were all in sync according to versions in this Bioconductor version. For all models except the DI-VNN, we used an R package of caret 6.0.86 that wraps R packages for the modeling algorithms, which were glmnet 4.1, Rborist 0.2.3, and gbm 2.1.8. For the DI-VNN, we used keras 2.3.0 and tensorflow 2.0.0 Python libraries via R packages of reticulate 1.16, keras 2.3.0.0, and tensorflow 2.0.0. We also created R packages and a Python library for many steps in the data analysis, including the DI-VNN, medhist 0.1.0, gmethods 0.1.0, rsdr 0.1.0, clixo 0.1.1, and divnn 0.1.3 (both an R package and Python library). All of these packages/libraries are available for download from this repository <a href=\"https://github.com/herdiantrisufriyana\" rel=\"noopener noreferrer\" target=\"_blank\">https://github.com/herdiantrisufriyana</a>. For model deployment, we used Shiny Server 1.4.16.958 and Node.js 12.20.0. Details on other R package versions and all of the source codes (vignette) for the data analysis are available (Table 6 in Supplementary Information).</p><p>\tTo reproduce our work, a set of hardware requirements may be needed. We used a single machine for all models, except the DI-VNN, with 16 logical processors for the 2.10 GHz central processing unit (CPU) (Xeon\u00ae E5-2620 v4, Intel\u00ae, Santa Clara, CA, USA), 128 GB RAM, and 11 GB graphics processing unit (GPU) memory (GeForce GTX 1080 Ti, NVIDIA, Santa Clara, CA, USA). Parallelism was applied for CPU computing. Meanwhile, the DI-VNN required a higher GPU capability than that provided by the previous specification. For hyperparameter tuning and training, we used multiple machines in a cloud with 90 GB RAM and 32 GB GPU memory (Tesla V100-SXM2, NVIDIA, Santa Clara, CA, USA). For predictions, the DI-VNN only needed a CPU in a local machine, or that in a cloud machine for the web application.</p>"
        },
        {
            "header": "Procedure",
            "content": "1. Choose the study design.\nEither prospective or retrospective cohort paradigm was recommended to prevent temporal bias for prognostic prediction, and case-control should be avoided.\n21\nThe latter study design may introduce collider-stratification or selection bias.\n13\nFor diagnostic prediction, a cross-sectional design was recommended by PROBAST.\n17\nThese guidelines also warned utilizing randomized-controlled trial for prediction study. Study design should reflect similar situation in population level according to whether we develop either prognostic or diagnostic prediction. In our example, a retrospective design was applied to select subjects from database.\n2. Define target population by selection criteria.\nSelection criteria of subjects with any outcome should be defined. These are not defined for each of outcome, i.e. case and control group; otherwise, we implicitly apply case-control design. In our example, all 12~55-year-old females were included, particularly whose visits were recorded within the dataset period. This represented population of visitors of the healthcare providers, since we intended to use our models for this population. Each period of pregnancy of the same subject should be treated as different subjects. The medical history before or during the first pregnancy was retained for the second instance. All visits after delivery should be excluded within the dataset period. Codes of diagnosis and procedure for determining delivery or immediately after delivery care should be determined and listed in supplementary information.\n3. Determine the type of prediction task.\nMachine learning prediction tasks may be either classifying a categorical outcome or estimating a numerical outcome, and either prognostic or diagnostic prediction. The estimation task was also commonly known as regression task. We avoid this word to prevent confusing it with regression algorithms which can also serve for both classification and estimation tasks. In our example, we determined our tasks were prognostic classification of prelabor rupture of membranes (PROM) and prognostic estimation of the time of delivery.\n4. Determine outcome definition.\n4. a. Define outcome for the classification task.\nIn the context of electronic health records, as shown in our example, we assigned one or more codes of diagnosis and/or procedure for an outcome based on International Classification of Disease version 10 (ICD-10) coding system. A subject was otherwise assigned as a nonevent. In the context of pregnancy, we may utilize the same codes for determining the end of the pregnancy period.\n4. b. Define outcome for the estimation task.\nThis would be an infinite set of numbers. In the context of pregnancy, as shown in our example, we did not have information about gestational age. But, we may infer the number of days from a visit, at which the time of prediction, to the last visit at which the time of true outcome. We used the same codes for the classification task to determine such visits encountering those of events and nonevents.\n5. Preserve censoring information in the dataset.\nCensoring outcome should not be excluded at first. For example, we may not know whether the subjects would be pregnant or not, and those who pregnant would be delivered or not up to the end of the study period. Instead of removing instances with censored outcomes, we assigned censoring labels. These were taken into account for causal inferences, and weighting of uncensored outcomes over both censored and uncensored outcomes when training the models later. Indeed, none of the censored outcomes would be included for developing prediction models. This would preserve similar distributions of any outcomes with those in the target population and resolve the class imbalance problem by inverse probability weighting.\n6. Set priority based on assessment of practical costs of under- and over-prediction.\nTo evaluate a prediction model, practical costs of prediction errors should be considered.\n16\nWe need to relate potential consequences of under- and over-prediction. Then, we need to choose which one would be likely more frequent or larger magnitude. A priority on dealing with under-prediction would need well calibration and higher true positive rate, while that of over-prediction would also need well calibration but higher specificity. For estimation task, we need to set limit of maximum error which is considerably safe to use a prediction model. In our example, we set priority on dealing with under-prognosis and limit of error within 2 to 4 weeks of the true time of delivery.\n7. Determine candidate predictors.\nWe need to collect data for variables classified into two groups. The first group were data for baseline variables, i.e. demographics. Meanwhile, the other variables were candidate predictors. We avoid using baseline variables for candidate predictors, particularly those which need to use private data and reflect social and economic background. However, we need to understand the characteristics of our population based on the baseline variables. Using these variables, we may find whether future, unobserved data have similar characteristics, since this would describe how likely the predictive performance of our models for those new instances. Baseline variables were indeed still included for causal inferences. We also avoided maternal age, although this variable is often a strong predictor. The reason why we avoid the variable was that machine learning models often memorize age non-linearly. In turn, a large weight is often assigned to maternal age followed by weight shrinking of other predictors. All of these demographic variables were respectively assigned either 0 or 1 for no or yes in each category of each variable. Meanwhile, we extracted ICD-10 codes for diagnoses or procedures from medical histories.\n8. Define and verify causal factors as parts of candidate predictors.\nWe made gmethods 0.1.0, an R package, that allows future investigators to conduct statistical tests for causal inference. Please kindly follow the protocol.\n9\nAfter verifying causal factors, we only included those in a prediction model that applied a logistic regression with a shrinkage method, as recommended by PROBAST, instead of using a stepwise selection method.\n17\nWe chose an RR, which applies L\n2\n-norm or beta regularization, because this method retains all causal factors within the model after weights are updated by training.\n22\nWe understood that this model would not necessarily be the best model, because of the use of causal factors. Predictive modeling normally exploits confounders to achieve better performances, while causal models cannot explain all variations among individuals, to which confounding factors contribute.\n13\nHowever, by comparing a predictive model to one that uses only causal factors, we could imagine how much confounder effects were exploited to improve the predictive performance by machine learning algorithms. This, in turn, can warn a human user of machine learning algorithms about conducting a critical appraisal of internal properties of a machine learning model. We followed the same procedures for hyperparameter tuning and parameter fitting (training) as those for machine learning, as described in the following sections. Nonetheless, we viewed tuning and training of this prediction model as already a part of machine learning since fewer interventions are required by human users.\n9. Remove candidate predictors with perfect separation problems.\nWe identified candidate predictors that took a single value or had zero variance. We removed all candidate predictors which were positive (value of 1) in only one of the outcomes in a training set. This is a perfect separation problem.\n16\nA predictor may be exclusive for one of the outcomes by chance due to sampling error. To prevent such a bias, we removed perfect-separation candidate predictors. Details of the candidate predictors and selection should be described in supplementary information.\n10. Remove candidate predictors that may cause outcome leakage.\nWe need to identify these kinds of candidate predictors. In the context of electronic health records, these would be any variable indicated by codes of diagnosis and procedure, which are explicitly or implicitly following the outcome definition. In our example, these referred to maternal or baby diagnosis/procedure codes that typically occur only during the delivery or post-delivery period. Otherwise, these codes would unexpectedly leak outcome information. The excluded codes should be listed and reported in supplementary information.\n11. Filter irredundant candidate predictors.\nTo conduct this step, we need to compute pair-wise Pearson correlation coefficients. A pair of candidate predictors which has a high correlation (i.e., >~0.70) should be removed. This may be done by using one of the paired of candidate predictors, or unifying both under a single definition. In our example, we had pairs of candidate predictors that were highly correlated, but, the coefficients were borderline (i.e. ~0.7), while those were causal factors and some codes defining the factors themselves. Yet, we interpreted the meaning was not considerably the same; thus, we passed those candidate predictors.\n12. Construct provider-wise datasets for model development and validation.\nWe used medical histories and causal factors as candidate features/predictors. Instead of using nationwide medical histories, we need to extract the provider-wise ones by estimation. We only considered medical history of a subject recorded in a single healthcare provider and treated that of the other providers as separated medical histories, like in real-world setting.\n13. Quantify medical histories with the Kaplan-Meier (KM) estimator.\nElectronic health records across healthcare provider are unlikely connected. We need to have nationwide historical (KM) rates for each code, derived from the training set only. All medical histories in days were transformed into historical rates. This technique allowed generalization of individual data based on nationwide, population-level data, without the need to access data from other providers. We made medhist 0.1.0, an R package, that allows future investigators to implement this historical rate. Please kindly follow the protocol.\n10\n14. Conduct dimensional reduction by resampling for candidate features of the machine-learning prediction models.\nWe made rsdr 0.1.0, an R package, that allows future investigators to apply resampled dimensional reduction. Please kindly follow the protocol.\n11\nResampling is important to prevent overfitting in machine learning.\n8\nBut, this is commonly applied in supervised instead of unsupervised machine learning; thus, this procedure is expected to deal with this problem.\n15. Consider the number of events per variable when choosing the number of candidate predictors for each model.\nThe minimum number is 20 events per variable (EPV), which was recommended for logistic regression, as recommended by the prediction model risk of bias assessment tools (PROBAST) guidelines.\n17\nDimensional reduction can be conducted to get latent variables fewer than the original candidate predictors. Furthermore, pre-selection may be applied by a regression model, which needs the least EPV. Then, we picked fewer latent variables with the highest absolute, non-zero weights as candidate predictors for the models which have higher EPV requirement, e.g. 200 EPV for random forest and gradient boosting machine.\n23\n16. Choose modeling approaches to be compared.\nFive modeling approaches were applied for supervised machine learning, consisting of a set of procedures covering feature representation, feature selection, hyperparameter tuning, and training strategies. For all models, we applied a grid search method for hyperparameter tuning of a minimum of 10 alternatives for each modeling approach. The best hyperparameters or tuning parameters were used for training the model or fitting the parameters.\n17. Compute outcome weights to overcome class imbalance problem.\nFor classification task, the outcome (\nY\n) was weighted by half of the inverse probability/prevalence (\nw\ni\n), including the censored outcome (\u2205). For example, if the prevalence of\nY = 1\nis 0.2, then\nw\ni\nis 1 \u00f7 0.2 \u00d7 0.5 for the outcome of\nY = 1\n.The sum of the three probabilities is equal to 1. The weight formula is shown as Figure 1. Weights were plugged into a general equation of the loss function in this study (Figure 2), in which training was generally conducted to estimate parameter\n\u03b8\nj\nin a model\nf(x\nij\n,\u03b8\nj\n)\nthat minimizes\nL\n, where\nn\nis the number of visits,\np\nis the number of candidate predictors,\nx\nij\nis the value for each\ni\nth\ninstance and\nj\nth\ncandidate predictor, and\n\u03b1\nand\n\u03bb\nare regularization factors. Meanwhile, no weighting was applied in the estimation task (\nw = 1\nfor any\ni\n). In addition, a specific training strategy was applied for the last modeling approach, which was the DI-VNN, a model using the pipeline we developed in a previous protocol.\n12\n18. Determine hyperparameters for the human-learning prediction model.\nThe first model was the causal RR. We used all causal factors except ones that included demographics. This model applied a filter method for feature selection by verifying assumptions based on domain knowledge with a statistical test for the causal inference, as described in the previous section. The RR was applied as the parameter-fitting algorithm. The tuning parameter was\n\u03bb = {10\n-9\n,10\n-8\n,\u22ef,100}\nwhile keeping\n\u03b1\u00a0= 0\n. These\nvalues were plugged into the loss function (Figure 2).\n19. Determine hyperparameters for the machine-learning prediction model using regression algorithm.\nThe second model was PC-ENR (elastic net regression). We used all PCs for the second model since the EPV was already >20 using the training set. This number, instead of 10, is recommended when applying a regression algorithm.\n17\nThis model also applied a shrinkage method for feature selection. Tuning parameters were combinations of\n\u03b1 = [0,0.25,\u22ef,1]\nand\n\u03bb = [10\n-9\n+g(10\n0\n-10\n-9\n)/G,\u22ef,10\n0\n]\nfor\ng = [1,2,\u22ef,G]\nand\nG = 5\n; thus, the best tuning parameters were searched over\n5 \u00d7 5\nalternatives. These values were plugged into the loss\nfunction (Figure 2), where\n\u03b1 = 0\nmeans this model becomes an RR but using PCs instead of the original candidate predictors. But if\n\u03b1 > 0\n, some candidate predictors\nx\n(j)\nmight be zero after being multiplied by\n\u03b8\nj\nwhich minimizes the training loss. Each time, this was applied to different\n\u03b1\nand\n\u03bb\nvalues to find a couple of values that minimized the validation loss.\n20. Conduct further pre-selection of candidate features for machine-learning prediction with larger number of events per variable.\nThe 3\nrd\nand 4\nth\nmodels respectively applied the PC-RF (random forest) and PC-GBM (gradient boosting machine). These models also applied the wrapper method for feature selection. This means that candidate predictors were pre-selected by a model before being used for these models. The wrapper model was the PC-ENR. We expected a smaller number of PCs to be pre-selected; thus, the EPV was \u2265200 using the training set. This is allowed by either using the wrapper method only or subsequently applying a filter method. The filter method was conducted by ranking the PCs in descending order based on the corresponding\n\u03b8\nj\n. We selected\nl\nof PCs where\nl\nis a number such that the EPV is 200. Unlike regression algorithms, modern machine algorithms are data hungry, of which the tested algorithms were the classification and regression tree (CART), RF, support vector machine, and shallow neural network.\n23\n21. Determine hyperparameters for the machine-learning prediction model using ensemble algorithms.\nRF and GBM are ensemble algorithms. This means that both use prediction results of multiple models that apply the same algorithm, i.e., CART. However, the RF ensembles models in parallel, while the GBM sequentially ensembles the models.\n24,25\nParallel ensemble means predictions are respectively the majority and average of CART predictions for classification and estimation tasks. Meanwhile, sequential ensemble means a simple CART is made to predict the classification or estimation error of an earlier CART model. Tuning parameters for the RF were combinations from a number of random candidate predictors used to build a CART each time\n[5+g(45-5)/G,\u22ef,45]\nand a number of minimum unique instances per node for\nG = 5\n, while 500 CARTs were built for each of the candidate models. For the GBM, we set tuning parameters as combinations from a number of CARTs\n[100,200,\u22ef,2500]\nand a shrinkage factor or L2-norm regularizer\n[0.0005+g(0.05-0.0005)/G,\u22ef,0.05]\nfor\nG = 25\n, while the minimum sample number per node (tree) was 20 and only 1 random predictor was used to build a CART each time. Since exhaustively comparing all possible combinations is time consuming, if not impossible, all numbers for both ensemble models were determined based on common practices for simplicity. However, we considered the sample size, the number of candidate predictors, and the diversity of approaches between the two algorithms to heuristically optimize the hypothesis search. The prediction results of these models were plugged into the loss function (Figure 2) with\n\u03b1 = 0\nand\n\u03bb = 0\nto compute the errors that were minimized by training.\n\ufeff\n22. Develop and validate DI-VNN model.\nThis model works like neurons in the brain. Predictors are fed as inputs into neurons, and the outputs follow an all-or-none activation function. These inputs are arrayed as a minimum of two-dimensional imaging such as an object projected onto retina. We called this an ontology array. A predictor that is correlated to another predictor would have a closer position than another less-correlated predictor. From the receptive field on the retina, the signals propagate to the primary visual cortex and then the visual association cortex. The signal pathway of the neural network is dedicated for specific parts of the array. We called this an ontology network. By seeing similar objects with uncertain variations, the neural network is maintained at specific activation thresholds and weights. This creates visual memory in the association cortex to recognize a particular object by segmenting it into several parts. We made divnn 0.1.3, an R package and Python library, that allows future investigators to develop and validate this model. Please kindly follow the protocol.\n12\n23. Evaluate all prediction models.\n23. a. Calculate 95% confidence interval from all resampling subsets.\nWe needed to evaluate our models in terms of both classification and estimation tasks. The 95% confidence interval (CI) was used to express all evaluation metrics as estimates. That interval was calculated from the metrics of multiple resampling subsets for model validation.\n23. b. Assess whether a model is well-calibrated for classification task.\nCalibration measures were evaluated by fitting the predicted probabilities to true probabilities using a univariable linear regression; thus, the calibration measures were the intercept and slope of this linear regression. We also demonstrated a calibration plot. A model is well-calibrated if the interval of the calibration intercept and slope respectively fall closer onto 0 and 1, and the calibration plot approximately hugs the reference line.\n23. c. Compare discriminative ability among well-calibrated models for classification task.\nMeanwhile, the discriminative ability of a model was quantified by an AUROC interval estimate. A non-overlapping, higher interval of the AUROC determined the best model among the most calibrated ones.\n23. d. Computer prediction error for estimation task.\nFor estimation, we applied the root mean squared error (RMSE) to train the models. However, since a longer interval is reasonably more difficult to predict, a single evaluation by RMSE might be insufficient to evaluate the estimation performance. In our example, we also evaluated the estimation plot between the predicted and true times of delivery, binned per week.\n23. e. Consider to differentiate prediction error of estimation based on predicted classification.\nDifferent lines in the estimation plot were provided for positive and negative predictions by the best classification model. This would be reasonable for clinical applications by giving an interval estimate of days for the true time of delivery conditional on the predicted one. In our example, we also limited this evaluation to an estimated time of 42 weeks or less, which is the maximum duration of pregnancy.\n23. f. Compare predictive performance for estimation task considering clinical relevance.\nTo determine the best estimation model, we computed a proportion of weeks in which each predicted time, in weeks, was included within an interval estimate of the true one. The interval had to be the maximum \u00b1\nx\nweeks when predicting >\nx\nweeks. In our example, if a model predicted that a woman would deliver in 6 weeks, then the interval estimate of the true time of delivery should be the maximum \u00b1 6 weeks. For any women predicted to deliver in 6 weeks, this number should fall into that interval.\n23. g. Limit minimum and maximum values that can be estimated precisely.\nIn our example,\nf\nor the best estimation model, we determined the minimum and maximum predicted times of delivery with acceptable precision for each predicted outcome of PROM based on a visual assessment using internal validation. We also computed the RMSE within this acceptable range.\n24. Assess fulfillment of success criteria of the model development.\nSuccess criteria of the modeling should be at least by a threshold-agnostic evaluation metric which is better than those of recent models of the similar outcome using similar predictors. Consider to add a criterion or more to prevent overfitting, as recommended by PROBAST guidelines.\n17\nIf there is no previous studies to be compared with, AUROC 0.8 or more can be considered for success criterion.\n18\n25. Compare to models from previous studies.\n25. a. Determine the context of model comparison across studies.\nModel comparison should be within the context of its clinical application. This is represented in the eligibility criteria to select studies to be compared with.\n25. b. Set up comparison guidelines.\nEvaluation of success criteria needs comparator models. To find these models, one can apply PRISMA 2020 guidelines; thus, the models are comparable and the comparison is also fair. Since a systematic review and meta-analysis were not the main purposes of a study in our example, we only applied all items in section methods in the guidelines, except item numbers 11 and 14 regarding the risk of bias assessment and item numbers 13 and 15 regarding synthesis method and certainty. This was because we applied the guidelines only to facilitate fair comparisons to previous studies, and did not consider conclusions as to how valid and accurate PROM could be predicted.\n25. c. Define study selection criteria.\nThe eligibility criteria should follow the PICOTS framework: (1) population, targeted subjects that may or may not have the outcome; (2) index, a prediction model of interest; (3) comparator, previous prediction models to be compared with; (4) outcome, definition and data scale for the outcome with or without additional criterion on the sample size (e.g. events per variable); (5) time, prognostic or diagnostic prediction with specified time interval; and (6) setting, healthcare setting such as primary care, hospital, inpatient, outpatient,\net cetera\n. In our example, only original articles and conference abstracts full papers were included. No grouping of the studies was needed because we would not conduct a meta-analysis for data synthesis.\n25. d. Develop the search strategy.\nA search date interval and literature databases should be defined. We also need to describe the keywords. Some configuration for searching studies should also be described, e.g., limiting publication date and language.\n25. e. Develop the review strategy.\nWe need to describe which author does what step of review. If there is a conflicting decision among reviewers, then how to achieve final decision should be described.\n25. f. Choose the extracted data.\nIn our example, we extracted evaluation metrics of the best model with the most similar outcome definition from each study to that of our study. Any data that are needed to briefly assess potential risk of bias were also extracted. These included the study design, population, setting, outcome definition, sample size, details on events and nonevents, number of candidate predictors, EPV, predictors in the best final model, and the most recommended validation techniques (external over internal validation; bootstrapping over cross-validation, or cross-validation over test split). If no AUROC was reported by a previous study, but there were sensitivity and specificity, then we computed AUROC from both of the metrics using trapezoidal rule (Figure 3).\n25. g. Define model evaluation method.\nPlots of evaluation should be overlaid those of our models. In our example, we plotted a point at a sensitivity and a specificity for each of our models at the optimum threshold on each ROC curve. AUROCs were also compared with the models in our example (with or without the 95% CI).\n26. Calibrate the predicted probability of any model for classification task.\nA model can be calibrated by training an additional model to estimate the true probability of an outcome for classification task given the predicted probability by the model. In our example, we applied a general additive model using locally weighted scatterplot smoothing (GAM-LOESS). Instead of using pre-calibrated models, the final comparison was made for models calibrated using predicted probabilities from each of the developed models. This calibration was intended to obtain more-linear predicted probabilities. No calibration was conducted for the estimation models.\n27. Validate the predictive performances using several data partition methods.\n27. a. Conduct data partition for non-randomized, external validation.\nWe split the dataset into two subsets which were intended for internal and external validation. For all external validation sets, evaluation metrics were resampled by bootstrapping 30 times to obtain interval estimates. As recommended, we split out a dataset for external validation by geographical, temporal, and geotemporal splitting. Geographical splitting was conducted by stratified random sampling of cities with as many as\nproportion of cities in each state. Temporal splitting was also conducted by stratified random sampling of days with as many as\np\nproportion of days in each subtropical season. Visits of subjects from either the sampled cities or days were respectively excluded as either a geographical or temporal subset. Then, we further excluded as many as\np\nproportion of cities in the geographical subset or that number of days in the temporal subset. Visits of subjects from both of the excluded subsets were overlapped as a new geotemporal subset. Either a geographical or geotemporal subset did not include visits or subjects in this geotemporal subset. We randomly tried different\np\nproportions for geographical, temporal, and geotemporal splitting such that approximately ~20% of visits belonged to any of the three subsets. These external validation sets were used for stress tests of our models since the distributions of predictors and outcome were conceivably uncertain among excluded cities, days, or overlaps. This reflects the situation in some real-world settings, but this might not reflect common situations nationwide.\n27. b. Conduct data partition for randomized, external validation.\nTo estimate the predictive performance nationwide, ~20% of the remaining set was randomly split out thus leaving ~64% of the original sample size for the training set. This random subset was more representative for external validation to estimate the predictive performances of our models nationwide. After splitting out this random subset, the remaining samples were intended for internal validation.\n27. c. Conduct data partition for calibration.\nWe split out ~20% of the internal validation set to calibrate each model. The final predictive performance of internal validation came from this calibration subset. We resampled it by bootstrapping 30 times to compute interval estimates. To compute the EPV, we only used ~80%, which was, the pre-calibrated subset. This was used to train the models.\n27. d. Choose resampling techniques for any validation.\nFor hyperparameter tuning, we applied 5-fold cross-validation, while the final training was conducted using the best tuned parameters by bootstrapping 30 times. Both tuning and training used the same pre-calibrated set by applying cross-validation for all models except the DI-VNN. Please kindly follow the protocol of DI-VNN.\n12\n28. Consider to assess the best time window for the best model of prognostic prediction.\nThe best time window should be evaluated using the best model for prognostic prediction. This provided additional insights when interpreting the best model. In our example, using the internal validation set, we grouped samples by binning the days to the end of pregnancy every 4 weeks. An interval estimate of the AUROCs was computed for each bin from all resampled subsets. By observing the plot, the best time window should mostly cover interval estimates that were greater than an AUROC of 0.5, which represents prediction by simple guessing.\n29. Deploy the best model for each type of tasks.\n29. a. Determine input for the deployed models.\nThe best model can be deployed for both classification and estimation tasks. A model can be quite complex to apply without a software/application; thus, a web application need to be provided for using the best models. This can be made using R Shiny. In our example,\na user needs to upload a deidentified, two-column comma-separated value (CSV) file of admission dates and ICD-10 codes of the medical history of a subject. A use case should be described with an example dataset for immediate demonstration of how to use the web application.\n29. b. Design user interface.\nFor\nresponsible clinical prediction,\na\nthreshold is expected to be flexible,\nset by a user. For the classification performance, a threshold should be able to be chose under expected population-level performances of evaluation metrics of interest. For the estimation performance, a true value should also be estimated based on a subpopulation with the same estimated value. This may also be conditioned on the same classification result. All population- and subpopulation-level metrics are expressed as interval estimates. In our example, the population data for the classification task were those of the calibration split, while those for the estimation task were data of the internal validation set which consisted of both pre-calibrated and calibrated subsets. In addition, for estimation task, a timeline should also be shown to visualize each predicted value with the true interval estimate at the subpopulation level. The timeline should show the time of prediction and those code entries that were used as features; thus, a medical history of a subject is visualized using this timeline. All results can be saved online and/or downloaded as a report. An example of the report should be shown. Inference durations 10 times should also be measured for a use case and reported as interval estimates."
        },
        {
            "header": "Troubleshooting",
            "content": "<p><strong>Step 1 to 6</strong></p><p><em><u>Problem</u></em></p><p>Failure to assign outcome</p><p><em><u>Possible reason</u></em></p><p>Absence of codes for outcome, or none in the target population</p><p><em><u>Solution</u></em></p><p>Consider to use other codes of outcome or get another dataset, or to change selection criteria for target population.</p><p><br></p><p><strong>Step 7</strong></p><p><em><u>Problem 1</u></em></p><p>Premature stop of computation</p><p><em><u>Possible reason 1</u></em></p><p>Dataset consists of a large sample size or number of variables</p><p><em><u>Solution 1</u></em></p><p>Consider to use computer with larger memory size or RAM. Alternatively, split table into ones consisting unique sets of variables.</p><p><br></p><p><em><u>Problem 2</u></em></p><p>No candidate predictors</p><p><em><u>Possible reason 2</u></em></p><p>Absence of codes for predictors</p><p><em><u>Solution 2</u></em></p><p>Consider to use other codes of predictors or get another dataset.</p><p><br></p><p><strong>Step 8</strong></p><p><em><u>Problem</u></em></p><p>No causal factor</p><p><em><u>Possible reason</u></em></p><p>Absence of codes for causal factor</p><p><em><u>Solution</u></em></p><p>Consider to use other codes of causal factor or get another dataset.</p><p><br></p><p><strong>Step 9 to 12</strong></p><p><em><u>Problem 1</u></em></p><p>No eligible candidate predictors</p><p><em><u>Possible reason 1</u></em></p><p>Perfect separation problem on all candidate predictors due to extremely small sample size of any class of outcome</p><p><em><u>Solution 1</u></em></p><p>Consider to use other codes of outcome or get another dataset.</p><p><br></p><p><em><u>Problem 2</u></em></p><p>Almost perfect prediction</p><p><em><u>Possible reason 2</u></em></p><p>Codes for outcome being those for predictors</p><p><em><u>Solution 2</u></em></p><p>Ensure these codes are not used for predictors.</p><p><br></p><p><em><u>Problem 3</u></em></p><p>Redundant candidate predictors</p><p><em><u>Possible reason 3</u></em></p><p>Codes for causal factor being those for other predictors</p><p><em><u>Solution 3</u></em></p><p>Consider to remove either causal factor or the other predictors, or assess if the context or meaning is different between those predictors.</p><p><br></p><p><strong>Step 13</strong></p><p><em><u>Problem</u></em></p><p>Unable to infer historical rate</p><p><em><u>Possible reason</u></em></p><p>A predictor is only available in an instance.</p><p><em><u>Solution</u></em></p><p>Remove this predictor.</p><p><br></p><p><strong>Step 14</strong></p><p><em><u>Problem</u></em></p><p>Unable to run dimensional reduction procedures</p><p><em><u>Possible reason</u></em></p><p>None or only 1&nbsp;candidate predictor with non-zero variance</p><p><em><u>Solution</u></em></p><p>Consider to use other candidate predictors.</p><p><br></p><p><strong>Step 15 to 17</strong></p><p><em><u>Problem</u></em></p><p>&lt;20 events per variable (EPV)</p><p><em><u>Possible reason</u></em></p><p>Too many candidate predictors or too small sample size of outcome</p><p><em><u>Solution</u></em></p><p>Apply pre-selection more strict, use other codes of outcome or get another dataset, or ensure models generalized in all the external validation sets.</p><p><br></p><p><strong>Step 18 to 22</strong></p><p><em><u>Problem</u></em></p><p>Premature stop of computation</p><p><em><u>Possible reason</u></em></p><p>Large dataset or complex model</p><p><em><u>Solution</u></em></p><p>Consider to use computer with larger GPU (DI-VNN) or CPU (other models) memory size. Alternatively, reduce batch size and adjust learning rate accordingly for DI-VNN, or reduce number of parallel processes for other models.</p><p><br></p><p><strong>Step 23</strong></p><p><em><u>Problem 1</u></em></p><p>Unable to compute calibration measures</p><p><em><u>Possible reason 1</u></em></p><p>Narrow range of predicted probabilities</p><p><em><u>Solution 1</u></em></p><p>Increase digits for decimal rounding.</p><p><br></p><p><em><u>Problem 2</u></em></p><p>Unable to compute receiver operating characteristic</p><p><em><u>Possible reason 2</u></em></p><p>Narrow range of predicted probabilities</p><p><em><u>Solution 2</u></em></p><p>Increase digits for decimal of thresholds.</p><p><br></p><p><strong>Step 18 to 19</strong></p><p><em><u>Problem</u></em></p><p>Slow processing</p><p><em><u>Possible reason</u></em></p><p>Dataset consists of a large sample size or complex architecture.</p><p><em><u>Solution</u></em></p><p>Consider to use computer with larger GPU memory size. Alternatively, increase batch size and adjust learning rate accordingly, stop training earlier, or reduce number of bootstrapping.</p><p><br></p><p><strong>Step 24 to 25</strong></p><p><em><u>Problem</u></em></p><p>No previous model to be compared with</p><p><br></p><p><em><u>Possible reason 1</u></em></p><p>New prediction problem</p><p><em><u>Solution 1</u></em></p><p>Consider to use AUROC 0.8 or more to be considered for success criterion.</p><p><br></p><p><em><u>Possible reason 2</u></em></p><p>No study found using the keywords or literature databases</p><p><em><u>Solution 2</u></em></p><p>Consider to add more relevant the keywords or literature databases.</p><p><br></p><p><strong>Step 26</strong></p><p><em><u>Problem</u></em></p><p>Unable to calibrate the models</p><p><em><u>Possible reason</u></em></p><p>Narrow range of predicted probabilities</p><p><em><u>Solution</u></em></p><p>Consider to calibrate using another algorithm, or not to calibrate the models.</p><p><br></p><p><strong>Step 27</strong></p><p><em><u>Problem</u></em></p><p>Incomplete outcome in a subset</p><p><em><u>Possible reason</u></em></p><p>Small sample size</p><p><em><u>Solution</u></em></p><p>Consider to increase proportion for the subset, or get another dataset.</p><p><br></p><p><strong>Step 28</strong></p><p><em><u>Problem</u></em></p><p>Unable to compute a metric for a period</p><p><em><u>Possible reason</u></em></p><p>Small sample size</p><p><em><u>Solution</u></em></p><p>Consider to widen the range of each period, or get another dataset.</p><p><br></p><p><strong>Step 29</strong></p><p><em><u>Problem</u></em></p><p>Slow prediction</p><p><em><u>Possible reason</u></em></p><p>Complex model</p><p><em><u>Solution</u></em></p><p>Reduce model complexity, or design interface that shows progress of prediction in multiple, short steps.</p>"
        },
        {
            "header": "Time Taken",
            "content": "<p><strong>All steps</strong></p><p>Approximate time: 1 hour to 8 days (pre-computed)</p><p><br></p><p><strong>Step 1 to 6</strong></p><p>Approximate time: 1 to 5 minutes (pre-computed)</p><p><br></p><p><strong>Step 7</strong></p><p>Approximate time: 5 to 15 minutes (pre-computed)</p><p><br></p><p><strong>Step 8</strong></p><p>Approximate time: 25 minutes to 3 hours per causal factor</p><p><br></p><p><strong>Step 9 to 12</strong></p><p>Approximate time: 1 to 5 minutes</p><p><br></p><p><strong>Step 13</strong></p><p>Approximate time: 1 to 2 minutes</p><p><br></p><p><strong>Step 14</strong></p><p>Approximate time: 5 to 30 minutes</p><p><br></p><p><strong>Step 15 to 17</strong></p><p>Approximate time: 1 to 2 minutes</p><p><br></p><p><strong>Step 18 to 23, and 26 to 28</strong></p><p>Approximate time: 1 to 20 hours per classification/estimation model</p><p><br></p><p><strong>Step 24 to 25</strong></p><p>Approximate time: 1 to 7 days</p><p><br></p><p><strong>Step 29</strong></p><p>Approximate time: 10 to 20 minutes</p>"
        },
        {
            "header": "Anticipated Results",
            "content": "<p>We may expect better generalization with data partition technique in this protocol. The predictive performance would be reasonably higher in random split by chance. Compared to non-random splits, that split includes more variation across healthcare facilities in a country. Some models apply overfitting strategy (i.e., random forest and gradient boosting machine) to outperform others but having lower generalization. This will expose to false decision in model selection. This may be prevented by evaluating models based on calibration set which is a different subset to that for training the models by the algorithm of interest.</p><p>\tAlthough causal RR uses causal factors, this does not mean this model would have the best predictive performance. A prediction model normally exploits confounders to predict all variations in the dataset. But, this model would be a good reference model for the others. We also may use this model to interpret important predictors in other models.</p><p>\tWe may or may not find any well-calibrated model. But, the most calibrated one(s) should be chosen among the prediction models, preferably those with distributions of predicted probabilities from 0 to 1. A wide range of predicted probabilities will help clinicians to widely adjust threshold based on local data distribution. The optimal threshold for the best model(s) should be given as initial threshold. In addition, distributions of predicted probabilities may or may not be visually differentiated between events and nonevents.</p><p>\tA well-calibrated prediction model may be either more sensitive or more specific based on receiver operating characteristic (ROC) curves. This should be assessed by internal validation (calibration split). A more-sensitive model (i.e. high sensitivity) at 95% specificity is considerably better for screening a disease from a population-level standpoint. But, sensitivity and specificity at the optimal threshold also need to be considered.</p><p>\tNon-random splitting may not describe generalization but this can confirm the robustness of the best model. The best model, which is chosen by internal validation, is expected to be the best in most, if not all, of the non-random splits of external validation. Yet, the random split may reflect common situations nationwide. We also need to ensure that the best model has interval estimates of the predictive performance better than simple guessing, i.e. the AUROC interval should be higher than 0.5 for all the non-random splits. This can be a safety factor to implement the best model before re-calibration using local data.</p><p>\tComparison to prediction models from previous studies is important to assess success model development. But, we may or may not find comparable models following methods in the preferred reporting items for systematic reviews and meta-analyses (PRISMA) 2020 expanded checklist.<sup>19</sup> In this situation, we may need to consider more literature databases or other keyword strategy. Furthermore, we may also consider modification of eligibility criteria. If we find the previous models, we need to compare using a threshold-agnostic evaluation metric (e.g. AUROC) along with additional metrics requiring a threshold and the sample size for each class of outcome. In addition, it is also important to point out if the predictors are those in either low- or high-resource settings.</p><p>\tFor estimation task, we need to consider clinical acceptance of the prediction error for each range of estimated values. Models may or may not fulfill the clinical acceptance for each of the ranges. DI-VNN may not estimate the true time of delivery in our example mainly due to the differential analysis. It only filters predictors based on categorical outcomes only. If classification and estimation tasks are related in a circumstance, as demonstrated by our example, we need to stratify the estimated value for each of the classes predicted by the best classification model. As for classification task, the best model for estimation task should also be confirmed to consistently outperform the other models using external validation sets.</p><p>\tSince an estimation model may not fulfill the clinical acceptance for each of the ranges, we need to determine the precise estimation window of the best estimation model using an internal validation set. This estimation window should be determined for each of the predicted classes. In addition, different estimated values for each of the class may gain insights to the outcome disease.</p>"
        },
        {
            "header": "References",
            "content": "<p>1. Sufriyana, H., et al. Comparison of Multivariable Logistic Regression and Other Machine Learning Algorithms for Prognostic Prediction Studies in Pregnancy Care: Systematic Review and Meta-Analysis. JMIR Med Inform 8, e16503 (2020).</p><p>2.&nbsp;Fleuren, L.M., et al. Machine learning for the prediction of sepsis: a systematic review and meta-analysis of diagnostic test accuracy. Intensive Care Med 46, 383-400 (2020).</p><p>3.&nbsp;Lee, Y., et al. Applications of machine learning algorithms to predict therapeutic outcomes in depression: A meta-analysis and systematic review. J Affect Disord 241, 519-532 (2018).</p><p>4.&nbsp;Gonem, S., Janssens, W., Das, N. &amp; Topalovic, M. Applications of artificial intelligence and machine learning in respiratory medicine. Thorax 75, 695-701 (2020).</p><p>5.&nbsp;Bien, N., et al. Deep-learning-assisted diagnosis for knee magnetic resonance imaging: Development and retrospective validation of MRNet. PLoS Med 15, e1002699 (2018).</p><p>6.&nbsp;Hannun, A.Y., et al. Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network. Nat Med 25, 65-69 (2019).</p><p>7.&nbsp;Rajpurkar, P., et al. Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists. PLoS Med 15, e1002686 (2018).</p><p>8.&nbsp;Wilkinson, J., et al. Time to reality check the promises of machine learning-powered precision medicine. Lancet Digit Health 2, e677-e680 (2020).</p><p>9.&nbsp;Sufriyana, H., Wu, Y.W. &amp; Su, E.C. Systematic human learning by literature and data mining for feature selection in machine learning. Protocol Exchange (2021).</p><p>10.&nbsp;Sufriyana, H., Wu, Y.W. &amp; Su, E.C. Quantifying medical histories with the Kaplan-Meier (KM) estimator for feature extraction of electronic health records in machine learning. Protocol Exchange (2021).</p><p>11.&nbsp;Sufriyana, H., Wu, Y.W. &amp; Su, E.C. Resampled dimensional reduction for feature representation in machine learning. Protocol Exchange (2021).</p><p>12.&nbsp;Sufriyana, H., Wu, Y.W. &amp; Su, E.C. Deep-insight visible neural network (DI-VNN) for improving interpretability of a non-image deep learning model by data-driven ontology. Protocol Exchange (2021).</p><p>13.&nbsp;Hern\u00e1n, M.A. &amp; Robins, J.M. Causal Inference: What If. , (Chapman &amp; Hall/CRC, Boca Raton, 2020).</p><p>14.&nbsp;Sharma, A., Vans, E., Shigemizu, D., Boroevich, K.A. &amp; Tsunoda, T. DeepInsight: A methodology to transform a non-image data to an image for convolution neural network architecture. Sci Rep 9, 11399 (2019).</p><p>15.&nbsp;Ma, J., et al. Using deep learning to model the hierarchical structure and function of a cell. Nat Methods 15, 290-298 (2018).</p><p>16.&nbsp;Luo, W., et al. Guidelines for Developing and Reporting Machine Learning Predictive Models in Biomedical Research: A Multidisciplinary View. J Med Internet Res 18, e323 (2016).</p><p>17.&nbsp;Moons, K.G.M., et al. PROBAST: A Tool to Assess Risk of Bias and Applicability of Prediction Model Studies: Explanation and Elaboration. Ann Intern Med 170, W1-w33 (2019).</p><p>18.&nbsp;Scott, I., Carter, S. &amp; Coiera, E. Clinician checklist for assessing suitability of machine learning applications in healthcare. BMJ Health Care Inform 28(2021).</p><p>19.&nbsp;Page, M.J., et al. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. Bmj 372, n71 (2021).</p><p>20.&nbsp;Huber, W., et al. Orchestrating high-throughput genomic analysis with Bioconductor. Nat Methods 12, 115-121 (2015).</p><p>21.&nbsp;Yuan, W., et al. Temporal bias in case-control design: preventing reliable predictions of the future. Nat Commun 12, 1107 (2021).</p><p>22.&nbsp;Van Calster, B., van Smeden, M., De Cock, B. &amp; Steyerberg, E.W. Regression shrinkage methods for clinical prediction models do not guarantee improved performance: Simulation study. Stat Methods Med Res 29, 3166-3178 (2020).</p><p>23.&nbsp;van der Ploeg, T., Austin, P.C. &amp; Steyerberg, E.W. Modern modelling techniques are data hungry: a simulation study for predicting dichotomous endpoints. BMC Med Res Methodol 14, 137 (2014).</p><p>24.&nbsp;Friedman, J. Greedy function approximation: A gradient boosting machine. Annals of Statistics 29, 1189-1232 (2001).</p><p>25.&nbsp;Breiman, L. Random Forests. Machine Learning 45, 5-32 (2001).</p>"
        },
        {
            "header": "Acknowledgements",
            "content": "<p>The social security administrator for health or <em>badan penyelenggara jaminan sosial (BPJS) kesehatan</em> in Indonesia gave permission to access the sample dataset in this protocol (dataset request approval number: 5064/I.2/0421). This protocol was funded by the Ministry of Science and Technology (MOST) in Taiwan (grant number MOST109-2221-E-038-018 and MOST110-2628-E-038-001) and the Higher Education Sprout Project from the Ministry of Education (MOE) in Taiwan (grant number DP2-110-21121-01-A-13) to Emily Chia-Yu Su.</p>"
        }
    ],
    "attributes": {
        "acceptedTermsAndConditions": true,
        "allowDirectSubmit": true,
        "archivedVersions": [],
        "articleType": "Method Article",
        "associatedPublications": [],
        "authors": [
            {
                "id": 53712164,
                "identity": "a89263eb-4287-42da-aff7-9d15c04ce3a7",
                "order_by": 0,
                "name": "Herdiantri Sufriyana",
                "email": "",
                "orcid": "https://orcid.org/0000-0001-9178-0222",
                "institution": "(1) Graduate Institute of Biomedical Informatics, College of Medical Science and Technology, Taipei Medical University, 250 Wu-Xing Street, Taipei 11031, Taiwan. (2) Department of Medical Physiology, Faculty of Medicine, Universitas Nahdlatul Ulama Surabaya, 57 Raya Jemursari Road, Surabaya 60237, Indonesia.",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Herdiantri",
                "middleName": "",
                "lastName": "Sufriyana",
                "suffix": ""
            },
            {
                "id": 53712165,
                "identity": "76894c99-2480-47bc-b3d1-f45235b91c84",
                "order_by": 1,
                "name": "Yu Wei Wu",
                "email": "",
                "orcid": "https://orcid.org/0000-0002-5603-1194",
                "institution": "(1) Graduate Institute of Biomedical Informatics, College of Medical Science and Technology, Taipei Medical University, 250 Wu-Xing Street, Taipei 11031, Taiwan. (2) Clinical Big Data Research Center, Taipei Medical University Hospital, 250 Wu-Xing Street, Taipei 11031, Taiwan.",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Yu",
                "middleName": "Wei",
                "lastName": "Wu",
                "suffix": ""
            },
            {
                "id": 53712166,
                "identity": "a20a3d05-7fc3-4175-a6c8-8ea715574db2",
                "order_by": 2,
                "name": "Emily Chia-Yu Su",
                "email": "emilysu@tmu.edu.tw",
                "orcid": "https://orcid.org/0000-0003-4801-5159",
                "institution": "(1) Graduate Institute of Biomedical Informatics, College of Medical Science and Technology, Taipei Medical University, 250 Wu-Xing Street, Taipei 11031, Taiwan. (2) Clinical Big Data Research Center, Taipei Medical University Hospital, 250 Wu-Xing Street, Taipei 11031, Taiwan. (3) Research Center for Artificial Intelligence in Medicine, Taipei Medical University, 250 Wu-Xing Street, Taipei 11031, Taiwan.",
                "correspondingAuthor": true,
                "prefix": "",
                "firstName": "Emily",
                "middleName": "Chia-Yu",
                "lastName": "Su",
                "suffix": ""
            }
        ],
        "badges": [],
        "createdAt": "2021-09-23 10:35:05",
        "currentVersionCode": 1,
        "declarations": "",
        "doi": "10.21203/rs.3.pex-1655/v1",
        "doiUrl": "https://doi.org/10.21203/rs.3.pex-1655/v1",
        "draftVersion": [],
        "editorialEvents": [],
        "editorialNote": "",
        "failedWorkflow": [],
        "files": [
            {
                "id": 15793640,
                "identity": "4bf9dc39-9fc8-4576-9454-e45f3aaa8d93",
                "added_by": "auto",
                "created_at": "2021-11-22 17:31:41",
                "extension": "png",
                "order_by": 1,
                "title": "Figure 1",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 347831,
                "visible": true,
                "origin": "",
                "legend": "Equation for weighting outcome.",
                "description": "",
                "filename": "hmlpfigure1.png",
                "url": "https://assets.researchsquare.com/files/pex-1655/v1/788e6526e86b8d7419108e6c.png"
            },
            {
                "id": 15793441,
                "identity": "a9dafbde-bb13-4ed5-b9a2-f54cf1d632f7",
                "added_by": "auto",
                "created_at": "2021-11-22 17:28:41",
                "extension": "png",
                "order_by": 2,
                "title": "Figure 2",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 760395,
                "visible": true,
                "origin": "",
                "legend": "Equation for loss function.",
                "description": "",
                "filename": "hmlpfigure2.png",
                "url": "https://assets.researchsquare.com/files/pex-1655/v1/ad55dd79e24e3e2cc6938fb8.png"
            },
            {
                "id": 15793639,
                "identity": "9b731e85-ddf5-4826-9425-144980d1853b",
                "added_by": "auto",
                "created_at": "2021-11-22 17:31:41",
                "extension": "png",
                "order_by": 3,
                "title": "Figure 3",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 746502,
                "visible": true,
                "origin": "",
                "legend": "Equation for computing AUROC from sensitivity and specificity by trapezoidal rule.",
                "description": "",
                "filename": "hmlpfigure3.png",
                "url": "https://assets.researchsquare.com/files/pex-1655/v1/5972b5944140bed9bd4e062d.png"
            },
            {
                "id": 15793641,
                "identity": "27c5c26f-6ab3-4593-a1f8-1b526e89c480",
                "added_by": "auto",
                "created_at": "2021-11-22 17:31:45",
                "extension": "pdf",
                "order_by": 0,
                "title": "",
                "display": "",
                "copyAsset": false,
                "role": "manuscript-pdf",
                "size": 1135918,
                "visible": true,
                "origin": "",
                "legend": "",
                "description": "",
                "filename": "manuscript.pdf",
                "url": "https://assets.researchsquare.com/files/pex-1655/v1/ee83bba0-b455-4fcd-8b1d-93c6fc4587c3.pdf"
            },
            {
                "id": 15793638,
                "identity": "2b8ea4d6-3d8f-435e-a177-f1dcd6f3a902",
                "added_by": "auto",
                "created_at": "2021-11-22 17:31:41",
                "extension": "pdf",
                "order_by": 1,
                "title": "",
                "display": "",
                "copyAsset": false,
                "role": "supplement",
                "size": 24051,
                "visible": true,
                "origin": "",
                "legend": "Supplementary Information",
                "description": "",
                "filename": "suppprotocolhmlp.pdf",
                "url": "https://assets.researchsquare.com/files/pex-1655/v1/72843adf4d4702a599749c5e.pdf"
            }
        ],
        "financialInterests": "",
        "fulltextSource": "",
        "fullText": "",
        "funders": [],
        "hasOptedInToPreprint": true,
        "hasPassedJournalQc": "",
        "hideJournal": true,
        "highlight": "",
        "institution": "",
        "isAuthorSuppliedPdf": false,
        "isDeskRejected": "",
        "isHiddenFromSearch": false,
        "isInQc": false,
        "isInWorkflow": false,
        "journal": {
            "display": true,
            "email": "protocol.exchange@nature.com",
            "identity": "protocol-exchange",
            "isNatureJournal": false,
            "hasQc": false,
            "allowDirectSubmit": true,
            "externalIdentity": "",
            "sideBox": "",
            "submissionUrl": "https://protocolexchange.researchsquare.com/submission",
            "title": "Protocol Exchange",
            "twitterHandle": ""
        },
        "keywords": "causal diagram, machine learning, deep neural network, medical history, electronic health record, dimensional reduction",
        "license": {
            "name": "CC BY 4.0",
            "url": "https://creativecommons.org/licenses/by/4.0/"
        },
        "manuscriptAbstract": "<p>This protocol aims to develop, validate, and deploy a prediction model using high dimensional data by both human and machine learning. The applicability is intended for clinical prediction in healthcare providers, including but not limited to those using medical histories from electronic health records. This protocol applies diverse approaches to improve both predictive performance and interpretability while maintaining the generalizability of model evaluation. However, some steps require expensive computational capacity; otherwise, these will take longer time. The key stages consist of designs of data collection and analysis, feature discovery and quality control, and model development, validation, and deployment.</p>",
        "manuscriptTitle": "Human and machine learning pipelines for responsible clinical prediction using high-dimensional data",
        "msid": "",
        "msnumber": "",
        "nonDraftVersions": [
            {
                "code": 1,
                "date": "2021-11-22 17:28:39",
                "doi": "10.21203/rs.3.pex-1655/v1",
                "editorialEvents": [
                    {
                        "type": "communityComments",
                        "content": 0
                    }
                ],
                "status": "published",
                "journal": {
                    "display": true,
                    "email": "info@researchsquare.com",
                    "identity": "researchsquare",
                    "isNatureJournal": false,
                    "hasQc": true,
                    "allowDirectSubmit": true,
                    "externalIdentity": "",
                    "sideBox": "",
                    "submissionUrl": "/submission",
                    "title": "Research Square",
                    "twitterHandle": "researchsquare"
                }
            }
        ],
        "origin": "",
        "ownerIdentity": "0c701e94-2fd4-4b1a-8893-957eeabc4181",
        "owner": [],
        "postedDate": "November 22nd, 2021",
        "published": true,
        "revision": "",
        "status": "posted",
        "subjectAreas": [
            {
                "id": 7394087,
                "name": "Information theory and computation"
            },
            {
                "id": 7394088,
                "name": "Computational biology and bioinformatics"
            },
            {
                "id": 7394089,
                "name": "Risk factors"
            },
            {
                "id": 7394090,
                "name": "Biomarkers"
            }
        ],
        "tags": [],
        "versionOfRecord": [],
        "versionCreatedAt": "2021-11-22 17:28:39",
        "video": "",
        "vorDoi": "",
        "vorDoiUrl": "",
        "workflowStages": []
    }
}