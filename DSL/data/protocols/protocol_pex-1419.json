{
    "identity": "pex-1419",
    "title": "<p>A research protocol of an observational study on efficacy of microsoft kinect azure in evaluation of static posture in normal healthy population</p>",
    "content": [
        {
            "header": "Introduction",
            "content": "<p>From a cybernetic perspective, our body system could be described as a network of structure and function based sub - systems with motor equiform within the principle of equilibrium, energy economy and contentment: thus, the optimal posture is something that provides full motor movement performance, and in absence of stress with maximal energy economy. The current research work is focused on the requirement of defining a real objective method of evaluating the postural variables, affordable and of easy use.<sup>5</sup></p><p>Human computer interaction (HCI) based on vision doesn't involve direct physical contact between users and apps. In this area, conventional approaches were generally based on regular sensors such as the RGB camera, which are not only computationally expensive but also easily affected by variations in brightness and background clutters.<sup>1</sup> Low-cost sensors such as Microsoft's Kinect II or Microsoft's Kinect azure sensor in 3D motion capturing systems showing a growing interest in vision-based HCI as an alternative to more expensive devices.<sup>1,2 </sup>(fig 1)</p><p>Micro-soft Kinect \u00ae, a gameplay device associated with Xbox console, has been the tool used throughout this analysis. Designed by Microsoft with in field of sport, through the Motion Capture Program, the kinect sensor \u00ae has fascinated millions of customers for years, capturing movements through cameras and instantaneous or delayed replay. The Kinect \u00ae seems to be an RGB lens, fitted via an infrared feature that captures 3D pictures.<sup>5</sup></p><p>Obtaining 3-dimensional body joint details is important for understanding the position of the human body however there seem to be several possible applications under which motion capture (MOCAP) systems can be used effectively. Two general methods being:&nbsp;marker-based methods where active (Light-emitting) or passive (reflective) markers could be used, and&nbsp;marker-less approach. Few camera based&nbsp;professional Motion - capture systems, like the Vicon or even Qualisys, have passive markers evident in infrared(IR) lenses, whereas marker-less processes do not need additional equipment in addition to cameras. For example, theCMU1'sOpenPosealgorithm analyses two-dimensional clips for joint approximation, but the Azure Kinect is using RGB and IR lenses to construct a three-dimensional value of the image.</p><p>Recently work on the recognition of human activity has been documented on systems showing strong overall success in recognition. Azure Machine can be used to construct up models of machine learning algorithms. Recognition output is based on: an activity set, data collection quality, feature extraction process, and learning algorithm.<sup>4</sup> The machine will calculate the distance of objects within the environment. The data from the sensor can be used in software applications by using a software development kit. The sensor provides information on the location of the recognized user joints in the frame in addition to the depth and color details.<sup>2</sup>&nbsp;</p><p>Advancement has been achieved this far in introducing new frameworks for understanding human posture through Kinect. Suma et al. suggested a versatile action with articulated skeletal toolkit called FAAST, that comprises 27 pre - determined human roles, such as LEAN LEFT, LEFT ARM UP, LEFT FOOT UP, and so on. When acquiring device skeleton data, the FAAST toolbox can execute virtual events such as mouse cursor control and keypad inputs, and can thus be used this to control virtual reality apps. Kang and some colleagues proposed a new way of controlling 3-dimensional application by collecting user commands through remote information, and also user position data via joints. Furthermore, Thanh et al. designed a system where robot can study human postures relying on the Semaphore method. Command terms were sent out letter by letter utilizing sign language throughout their approach. The area of physical posture recognition has been revamped by modern algorithms utilizing Kinect. While Kinect 's depth images consists&nbsp;complete three dimensional posture detail, body recognition takes a lot of effort to extract away unnecessary items that is not even component of human body. With implementation of NITE and also the Flexible Action and Articulated Skeleton Toolkit (FAAST), designed by Suma et al., depending on articulated skeleton sites, some simple pre - specified human actions such as swipe, loop, jump and hop may be recognized. These works build a solid basis for posture recognition research that use data from Kinect skeletons. Such aforementioned methods, with Kinect's support, have vastly improved existing human posture image recognition. But certain drawbacks hamper their success in general circumstances in terms of productivity and applicability. First, all of these strategies have quite constricted lexicon posture, and could only handle basic postures that are far from sufficient in advanced tasks. Second, such methods are mainly based on computer programmer-defined postures, which doubtlessly lessen an HCI system's versatility and applicability. Moreover, user-defined parameters are needed in these processes that may be subjective and cumbersome in real-world applications. To tackle these question, we suggest a newer method of recognizing human posture via&nbsp;adopting machine learning technique. Without empirical criteria this technique can instantly recognize a certain user-defined pose often with superior efficiency.<sup>5</sup></p><p>Human Posture Recognition can also be seen as a sort of sub-field for recognition of motion as a pose is a \"static motion\". The Kinect system features an infrared imaging scanner coupled via a monochrome CMOS sensor collecting information from images. This machine has an RGB lens, and even a microphone with several arrays. The Kinect system therefore gives us the opportunity to simultaneously record the image of the color and the description of the observable scene in detail. A skeleton monitoring feature was recently implemented in the latest version of Kinect SDK. This method is intended to store the joints as points compared to the apparatus itself. The knowledge of conjoints is contained in images. The locations of the various points are determined and extracted to each frame.<sup>.3</sup>(Fig 2)</p><p>We have three key details for each Joint. A joint has a certain discrete value for index. The first info is the Joints index. The 2nd knowledge is where each joint will be located in coordinates x, y, &amp; z. Those 3 coordinates are measured in metres. The Depth Sensor's body axis are the axes x, y, and z. It is a right-hand coordination device that positions the array of sensors at the beginning where the positive z axis extends in the way the series of sensors points in. A positive y dimension moves upward, and thus the positive x dimension falls to the left (for the sensor array).<sup>3</sup> (fig. 3). </p>"
        },
        {
            "header": "Reagents",
            "content": ""
        },
        {
            "header": "Equipment",
            "content": "<p>Consent form</p><p>Projector</p><p>Microsoft Kinect azure</p><p><br></p><p>Outcome measure: normal posture landmarks</p>"
        },
        {
            "header": "Procedure",
            "content": "Obtain informed consent, approach participants and screen for eligibility (N=132)\n\u21e9\nBaseline attention bias assessment (N=132)\n\u21e9\nOrientation to application of Kinect Azure for posture evaluation\n\u21e9\nAdministration and assessment of posture by Kinect Azure\n\u21e9\nStatistical Analysis\nWe will collect data of 132 participants (both male and female). They will be instructed to stand normally in such a way that frontal plane of posture comes under sensor coverage. Sensor will record and interpret the data with the markings of normal posture landmarks. The Kinect sensor is mounted at each experiment to captures the subject image. The distances are compatible with the recommendation for achieving the highest data quality.\nAlso, the age, weight, height, sex, and profession of the individuals (example undergraduate, businessman, dietician, or software developer) will be recorded as additional information for better annotation of the collected data. This data is useful in better understanding the results that need to be derived and in drawing conclusions from analysis."
        },
        {
            "header": "Troubleshooting",
            "content": ""
        },
        {
            "header": "Time Taken",
            "content": ""
        },
        {
            "header": "Anticipated Results",
            "content": "<p>The study\u2019s expected outcome will concert on evaluation of normal posture in healthy individual. After accomplishment of study result will be calculated by systemic data analysis by randomized control trial.&nbsp;</p><p><br></p>"
        },
        {
            "header": "References",
            "content": "<p><strong>References:</strong></p><p>1. \tZhang Z, Liu Y, Li A, Wang M. A novel method for user-defined human posture recognition using Kinect. In: 2014 7th International Congress on Image and Signal Processing. 2014. p. 736\u201340.&nbsp;</p><p>2. \tDiego-Mas JA, Alcaide-Marzal J. Using KinectTM sensor in observational methods for assessing postures at work. Appl Ergon. 2014 Jul;45(4):976\u201385.&nbsp;</p><p>3. \tLe T-L, Nguyen M-Q, Nguyen T-T-M. Human posture recognition using human skeleton provided by Kinect. In: 2013 International Conference on Computing, Management and Telecommunications (ComManTel). 2013. p. 340\u20135.&nbsp;</p><p>4. \t\u010curi\u0107 M, Kevri\u0107 J. Posture Activity Prediction Using Microsoft Azure. In: Had\u017eikadi\u0107 M, Avdakovi\u0107 S, editors. Advanced Technologies, Systems, and Applications II. Cham: Springer International Publishing; 2018. p. 299\u2013306. (Lecture Notes in Networks and Systems).&nbsp;</p><p>5. &nbsp; &nbsp; &nbsp; &nbsp; Jeong J, Wang Y, Shah M. A Low-Cost Motion Capture System&nbsp; using Synchronized Azure Kinect Systems. :1.</p><p><br></p><p><br></p>"
        }
    ],
    "attributes": {
        "acceptedTermsAndConditions": true,
        "allowDirectSubmit": true,
        "archivedVersions": [],
        "articleType": "Method Article",
        "associatedPublications": [],
        "authors": [
            {
                "id": 19329985,
                "identity": "444f36f5-4bdb-4e60-8303-1067f6393484",
                "order_by": 0,
                "name": "Tamanna Nurai",
                "email": "",
                "orcid": "https://orcid.org/0000-0002-0675-751X",
                "institution": "Intern, Ravi Nair Physiotherapy College, Datta Meghe Institute of Medical Sciences, Sawangi(Meghe), Wardha, Maharastra, India",
                "correspondingAuthor": false,
                "prefix": "",
                "firstName": "Tamanna",
                "middleName": "",
                "lastName": "Nurai",
                "suffix": ""
            },
            {
                "id": 19329986,
                "identity": "6a157cda-5f68-4a58-ab64-7985a3b55c4b",
                "order_by": 1,
                "name": "Waqar Naqvi",
                "email": "waqar.naqvi@dmimsu.edu.in",
                "orcid": "https://orcid.org/0000-0003-4484-8225",
                "institution": "Professor and Head of Department, Department of Community health Physiotherapy, Ravi Nair Physiotherapy College, Datta Meghe Institute of Medical Sciences Sawangi(Meghe), Wardha, Maharashtra, India",
                "correspondingAuthor": true,
                "prefix": "",
                "firstName": "Waqar",
                "middleName": "",
                "lastName": "Naqvi",
                "suffix": ""
            }
        ],
        "badges": [],
        "createdAt": "2021-03-22 05:39:55",
        "currentVersionCode": 1,
        "declarations": "",
        "doi": "10.21203/rs.3.pex-1419/v1",
        "doiUrl": "https://doi.org/10.21203/rs.3.pex-1419/v1",
        "draftVersion": [],
        "editorialEvents": [],
        "editorialNote": "",
        "failedWorkflow": [],
        "files": [
            {
                "id": 8812415,
                "identity": "bfaf234c-5060-49e0-9b89-8aae66025c87",
                "added_by": "auto",
                "created_at": "2021-05-05 16:43:12",
                "extension": "png",
                "order_by": 1,
                "title": "Figure 1",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 1953621,
                "visible": true,
                "origin": "",
                "legend": "Microsoft Azure",
                "description": "",
                "filename": "fig.1.png",
                "url": "https://assets.researchsquare.com/files/pex-1419/v1/f1246181a150b26957f5d5eb.png"
            },
            {
                "id": 8813087,
                "identity": "bf566b90-ed14-4408-ad17-2e4b7152b09a",
                "added_by": "auto",
                "created_at": "2021-05-05 16:49:12",
                "extension": "png",
                "order_by": 2,
                "title": "Figure 2",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 265723,
                "visible": true,
                "origin": "",
                "legend": "Joints points",
                "description": "",
                "filename": "fig.2.png",
                "url": "https://assets.researchsquare.com/files/pex-1419/v1/cf029d537547dc8fad48b93c.png"
            },
            {
                "id": 8812788,
                "identity": "909b0763-7fe7-4e9a-9c96-87ba135da076",
                "added_by": "auto",
                "created_at": "2021-05-05 16:46:12",
                "extension": "png",
                "order_by": 3,
                "title": "Figure 3",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 13674,
                "visible": true,
                "origin": "",
                "legend": "Three coordinates for joint position",
                "description": "",
                "filename": "fig.3.png",
                "url": "https://assets.researchsquare.com/files/pex-1419/v1/36e67bfbf54e2a1b3ef37894.png"
            },
            {
                "id": 8812412,
                "identity": "ced7191d-5595-40fd-a45f-20fb70eb43d5",
                "added_by": "auto",
                "created_at": "2021-05-05 16:43:12",
                "extension": "jpg",
                "order_by": 4,
                "title": "Figure 4",
                "display": "",
                "copyAsset": false,
                "role": "figure",
                "size": 62715,
                "visible": true,
                "origin": "",
                "legend": "Procedure",
                "description": "",
                "filename": "fig.4.jpg",
                "url": "https://assets.researchsquare.com/files/pex-1419/v1/9bcf5e9a16220d6c3602c38e.jpg"
            },
            {
                "id": 13691593,
                "identity": "97b8ad09-12ac-4192-93bc-168a132a1275",
                "added_by": "auto",
                "created_at": "2021-09-17 12:39:26",
                "extension": "pdf",
                "order_by": 0,
                "title": "",
                "display": "",
                "copyAsset": false,
                "role": "manuscript-pdf",
                "size": 1125498,
                "visible": true,
                "origin": "",
                "legend": "",
                "description": "",
                "filename": "manuscript.pdf",
                "url": "https://assets.researchsquare.com/files/pex-1419/v1/a3b311bd-748e-4971-9a0b-dc8e57b0ce7a.pdf"
            }
        ],
        "financialInterests": "",
        "fulltextSource": "",
        "fullText": "",
        "funders": [],
        "hasOptedInToPreprint": true,
        "hasPassedJournalQc": "",
        "hideJournal": true,
        "highlight": "",
        "institution": "",
        "isAuthorSuppliedPdf": false,
        "isDeskRejected": "",
        "isHiddenFromSearch": false,
        "isInQc": false,
        "isInWorkflow": false,
        "journal": {
            "display": true,
            "email": "protocol.exchange@nature.com",
            "identity": "protocol-exchange",
            "isNatureJournal": false,
            "hasQc": false,
            "allowDirectSubmit": true,
            "externalIdentity": "",
            "sideBox": "",
            "submissionUrl": "https://protocolexchange.researchsquare.com/submission",
            "title": "Protocol Exchange",
            "twitterHandle": ""
        },
        "keywords": "Kinect azure, human posture evaluation, coronal posture, healthy individuals, projector",
        "license": {
            "name": "CC BY 4.0",
            "url": "https://creativecommons.org/licenses/by/4.0/"
        },
        "manuscriptAbstract": "<p><strong>BACKGROUND: </strong>Recognition of human pose is very significant in studies involving human computer interactions. Microsoft's Kinect II or Microsoft's Kinect azure sensor in 3D motion capturing systems shows a growing interest in vision-based Human computer interaction as they are low-cost. In this research we introduced and ruled out the efficacy of Microsoft Kinect Azure in evaluation of static coronal Posture in normal healthy population.</p><p><strong>METHOD: </strong>The research has been structured as an observational study. The total of 132 participants will be taken from AVBRH, sawangi Meghe for study as per inclusion and exclusion criteria. With intervention the period of the study will be 6 months. It holds single period, concurrent validity evaluation comparing normal Posture derived from the Kinect system.</p><p><strong>DISCUSSION: </strong>This study protocol aims to evaluate the Validity of evaluation of Normal human posture using Microsoft Kinect Azure. The study's expected outcome will concert on the evaluation of coronal Posture using Microsoft Kinect Azure in normal healthy population.</p>",
        "manuscriptTitle": "A research protocol of an observational study on efficacy of microsoft kinect azure in evaluation of static posture in normal healthy population",
        "msid": "",
        "msnumber": "",
        "nonDraftVersions": [
            {
                "code": 1,
                "date": "2021-05-05 16:43:09",
                "doi": "10.21203/rs.3.pex-1419/v1",
                "editorialEvents": [
                    {
                        "type": "communityComments",
                        "content": 0
                    }
                ],
                "status": "published",
                "journal": {
                    "display": true,
                    "email": "info@researchsquare.com",
                    "identity": "researchsquare",
                    "isNatureJournal": false,
                    "hasQc": true,
                    "allowDirectSubmit": true,
                    "externalIdentity": "",
                    "sideBox": "",
                    "submissionUrl": "/submission",
                    "title": "Research Square",
                    "twitterHandle": "researchsquare"
                }
            }
        ],
        "origin": "",
        "ownerIdentity": "2004a935-df77-4d89-8fea-f35116aea6a0",
        "owner": [],
        "postedDate": "May 5th, 2021",
        "published": true,
        "revision": "",
        "status": "posted",
        "subjectAreas": [
            {
                "id": 4110518,
                "name": "Health humanities"
            },
            {
                "id": 4110519,
                "name": "Anatomy"
            },
            {
                "id": 4110520,
                "name": "Biotechnology"
            }
        ],
        "tags": [],
        "versionOfRecord": [],
        "versionCreatedAt": "2021-05-05 16:43:09",
        "video": "",
        "vorDoi": "",
        "vorDoiUrl": "",
        "workflowStages": []
    }
}